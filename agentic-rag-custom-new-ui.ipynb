{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95e3951-1a61-4926-b53e-5bf65d89cba5",
   "metadata": {},
   "source": [
    "# Agentic Self-Reflective RAG on Dell AI Factory with NVIDIA\n",
    "### with Elasticsearch vector database\n",
    "### Models served from K8s cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af8c91-1f14-4f64-8d64-4a9781f88066",
   "metadata": {},
   "source": [
    "<img src=\"images/agentic-rag-pipeline.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214c25e-344d-47a4-ac93-423b590f9a9f",
   "metadata": {},
   "source": [
    "## What is Agentic RAG?  \n",
    "\n",
    "LLM agents extend the capabilities of traditional LLMs by blending their natural language comprehension capabilities with actionable functionalities. This is a significant advancement for AI, making it ideal for automation and intelligent decision-making across industries. Unlike traditional LLMs, which generate text based solely on their training data, LLM agents can connect to external systems such as APIs, databases, and applications to fetch live data, provide contextually relevant responses, and combine them in pipelines to enhance their utility in real-world applications.\n",
    "\n",
    "\n",
    "This ability transforms LLMs from passive responders into dynamic actors capable of handling multi-step workflows and delivering actionable insights. In healthcare, for instance, LLM agents can securely synthesize information from patient records, clinical guidelines, and research databases to support timely, evidence-based decisions. These agents can assist in tasks such as patient diagnosis, treatment planning, and drug discovery, thereby enhancing the efficiency and accuracy of healthcare processes.\n",
    "\n",
    "\n",
    "The power to process and act on information in real time while adhering to stringent compliance standards positions LLM agents as powerful tools for addressing complex, data-intensive challenges. The agents redefine what AI can accomplish, providing scalable, secure, and contextually relevant solutions to some of the most demanding problems in modern industries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5be97b",
   "metadata": {},
   "source": [
    "# NVIDIA NIMs\n",
    "\n",
    "The `langchain-nvidia-ai-endpoints` package contains LangChain integrations building applications with models on \n",
    "NVIDIA NIM inference microservice. NIM supports models across domains like chat, embedding, and re-ranking models \n",
    "from the community as well as NVIDIA. These models are optimized by NVIDIA to deliver the best performance on NVIDIA \n",
    "accelerated infrastructure and deployed as a NIM, an easy-to-use, prebuilt containers that deploy anywhere using a single \n",
    "command on NVIDIA accelerated infrastructure.\n",
    "\n",
    "NVIDIA hosted deployments of NIMs are available to test on the [NVIDIA API catalog](https://build.nvidia.com/). After testing, \n",
    "NIMs can be exported from NVIDIA’s API catalog using the NVIDIA AI Enterprise license and run on-premises or in the cloud, \n",
    "giving enterprises ownership and full control of their IP and AI application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae712c52-b454-4523-95e9-7cc51df8d9f8",
   "metadata": {},
   "source": [
    "### About this notebook\n",
    "\n",
    "- Single LLM role play in a multi-agent set of tasks\n",
    "- Two data sources are used, RAG and a web search fall back, but more can be added to the query router.  Route A and B are available.  Route C is shown as an example.\n",
    "- NVIDIA NIMS are installed on a K8s cluster and accessed via API calls\n",
    "- Notebook does not need to be run on a GPU enabled machine, all GPU required services are provided by the K8s cluster.\n",
    "- Features code that can assist with clickable source files\n",
    "- Features a method to turn OFF the Agentic processes to show the different in results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ceb37-ee74-45da-b666-f12a2d7ccbb2",
   "metadata": {},
   "source": [
    "### Code credit and inspiration:\n",
    "- https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/#llms\n",
    "- https://github.com/NVIDIA/workbench-example-agentic-rag\n",
    "- David O'Dell\n",
    "- Tiffany Fahmy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d304f-dde1-45ed-9c86-eb45de74586b",
   "metadata": {},
   "source": [
    "# Library installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653986bb-82d7-4927-8032-f97771c9a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q langchain-nvidia-ai-endpoints==0.2.2\n",
    "# %pip install -q langchain==0.2.16\n",
    "# %pip install -q langchain-community==0.2.17                   \n",
    "# %pip install -q langchain-core==0.2.40\n",
    "# %pip install -q langchain-text-splitters==0.2.4\n",
    "# %pip install -q langchain-openai==0.1.23\n",
    "# %pip install -q pdfminer-six==20231228\n",
    "# %pip install -q pillow-heif==0.18.0\n",
    "# %pip install -q opencv-python==4.10.0.84 \n",
    "# %pip install -q unstructured==0.15.9\n",
    "# %pip install -q unstructured-pytesseract==0.3.12\n",
    "# %pip install -q pi-heif==0.18.0\n",
    "# %pip install -q unstructured-inference==0.7.36\n",
    "# %pip install -q tesseract==0.1.3\n",
    "# %pip install -q pytesseract==0.3.10\n",
    "# %pip install -q langgraph==0.2.15\n",
    "# %pip install -q gradio==4.27.0\n",
    "# %pip install -q elasticsearch==8.15.1\n",
    "# %pip install -q tiktoken==0.8.0\n",
    "# %pip install -q langchain-elasticsearch==0.2.2\n",
    "\n",
    "## Modified\n",
    "# %pip install -q gradio==5.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd2cb4-19b9-4f91-b77e-9f64f799a7cf",
   "metadata": {},
   "source": [
    "### Set debug and verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993a804c-9ec1-464e-9875-201a30aaa40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac01f3-238c-42da-92be-ed54adb5e591",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2ffb6b-813b-47a7-82d9-62001adc3c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    }
   ],
   "source": [
    "import nltk  \n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f7bc58-8680-418e-8279-af8715bd3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import loaders\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import CSVLoader\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_community.document_loaders.merge import MergedDataLoader\n",
    "\n",
    "### for embedding\n",
    "# from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "\n",
    "### status bars and UI and other accessories\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650977f5-c202-4aa7-bde3-212c94c86448",
   "metadata": {},
   "source": [
    "# Declare external services\n",
    "\n",
    "Services that will be hosted outside this application, usually the LLM, the vectordb and anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df82fc3-d5cc-4984-8b82-43114bbb8bb5",
   "metadata": {},
   "source": [
    "## Langsmith Tracing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c177e7-0f84-464f-946b-a1ba74988b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Consider adding these as env vars in AI Workbench to enable LangSmith tracing ###\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"nvd-agentic-RAG-test\"\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7154b39-4fdc-4a18-a48f-add9403c9fd2",
   "metadata": {},
   "source": [
    "### Define Local LLM for initial testing\n",
    "\n",
    "##### Model NIM, Embeddingn and Rerank will all have different ports.  In this case we used 30001, 30002, 30003. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b471ddc3-bd98-429e-80db-0f217604af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, NVIDIARerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a677f0-b851-4fce-a0cd-a1a9185992cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"meta/llama-3.1-8b-instruct\"\n",
    "model_id = \"meta/llama-3.1-70b-instruct\"\n",
    "api_url = \"http://llama31-70b-nim-nvidia-nim.apps.ai-ocp.emea.dsc.local/v1\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=api_url,\n",
    "    api_key=\"YOUR API KEY\", #API key for what? The NIM doesn't require API key?\n",
    "    model=model_id,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c23c53-6a15-4f0c-8cd6-d58f4d6943e0",
   "metadata": {},
   "source": [
    "### Define embeddings options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4142ec38-5c75-41a6-9ba6-ba68ee8bf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = NVIDIAEmbeddings(\n",
    "    base_url=\"http://www.nv-embedding.apps.ai-ocp.emea.dsc.local/v1\", \n",
    "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
    "    truncate=\"END\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39e5bf-56ad-4fee-823e-d57e3b552a34",
   "metadata": {},
   "source": [
    "### Define reranking options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6cefce-2f02-4885-a964-318820e3674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = NVIDIARerank(\n",
    "    base_url=\"http://www.rerankqa-mistral-4b.apps.ai-ocp.emea.dsc.local/v1\", \n",
    "    model=\"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "    truncate=\"END\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc6a7c-7ffd-40dd-856f-529ac60f2460",
   "metadata": {},
   "source": [
    "## Define Elasticsearch vector db instance\n",
    "\n",
    "using this as inspiration:  https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/generative-ai/chatbot.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d9f894c-a2fb-4011-a016-8a389e22b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from langchain_elasticsearch import ElasticsearchStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97396109-3f84-426f-8ed1-bea006cb5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set certificate permissions to 644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c3ac5b0-50f6-48fd-a8d4-d4e336bd8449",
   "metadata": {},
   "outputs": [],
   "source": [
    "CERTIFICATE = \"/home/demo/es_http_ca.crt\"\n",
    "HOST = \"https://www.elasticsearch.apps.ai-ocp.emea.dsc.local\"\n",
    "USER = \"elastic\"\n",
    "PASSWORD = \"dyak3z319h71u0Ch7y65avQs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec2b08b-b34e-47ba-a6e2-e918b69294b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/elasticsearch/_sync/client/__init__.py:400: SecurityWarning: Connecting to 'https://www.elasticsearch.apps.ai-ocp.emea.dsc.local:443' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    }
   ],
   "source": [
    "es_client = Elasticsearch(\n",
    "    hosts=HOST,\n",
    "    basic_auth=(USER, PASSWORD),\n",
    "    verify_certs=False,\n",
    "    # ca_certs=CERTIFICATE,\n",
    "    # verify_certs=False,\n",
    "    # ca_certs=False,\n",
    "    # ca_certs=True,    \n",
    "    # connection_class=RequestsHttpConnection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c41fc3-0892-4bdf-89f7-66749e2605ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<Elasticsearch(['https://www.elasticsearch.apps.ai-ocp.emea.dsc.local:443'])>\n",
      "{'name': 'elasticsearch-sample-es-default-0', 'cluster_name': 'elasticsearch-sample', 'cluster_uuid': 'WkoIxxUjSP-XDtgB1H_KCg', 'version': {'number': '8.16.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '12ff76a92922609df4aba61a368e7adf65589749', 'build_date': '2024-11-08T10:05:56.292914697Z', 'build_snapshot': False, 'lucene_version': '9.12.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.elasticsearch.apps.ai-ocp.emea.dsc.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.elasticsearch.apps.ai-ocp.emea.dsc.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(es_client.ping())\n",
    "print(es_client)\n",
    "print(es_client.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b263cf-798e-49dd-8249-745af64cf7a9",
   "metadata": {},
   "source": [
    "# Vector db Content setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f81069-4f5b-4e40-ba5a-cf27f6c18039",
   "metadata": {},
   "source": [
    "### Load PDF data into loader object\n",
    "\n",
    "##### We want the pdf files to be clickable so we set up a prefix appended to each file that points to the server they are residing at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a411985f-9911-491c-9b50-5c89e5b3d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing PDF files\n",
    "pdf_directory = \"docs\"\n",
    "# url_prefix = \"http://IP ADDRESS OF FILE SERVER/\" #Where do we get the pdfs? Would be easier to put them locally for time being and making prefix empty\n",
    "url_prefix = \"\"\n",
    "\n",
    "# Load PDF documents\n",
    "pdf_dir_loader = PyPDFDirectoryLoader(pdf_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cfbb65-a495-442d-88f1-8d0853917096",
   "metadata": {},
   "source": [
    "### Load CSV data into loader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b1ea245-e5b6-4c4f-9411-4c49f5f961cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data_csv_loader = CSVLoader(\"docs/healthcare_dataset.csv\", encoding='windows-1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381133d5-246c-4090-adc5-6ff830cfa474",
   "metadata": {},
   "source": [
    "### view CSV head contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44eac0cc-a7c1-4f49-a4fa-601b1de9f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d359ac38-62d7-4052-b911-31f6947319ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patient Name Date of Birth  Age Reason for Admission       Procedure  \\\n",
      "0    Patient_3      8/6/2004   19                COVID  antiviral meds   \n",
      "1   Patient_39     3/19/2004   19                COVID  oxygen therapy   \n",
      "2   Patient_33     12/8/2002   21                COVID  oxygen therapy   \n",
      "3   Patient_40      7/6/2001   23                  Flu  antiviral meds   \n",
      "4    Patient_1    10/28/1999   24            Pneumonia     antibiotics   \n",
      "\n",
      "       Room Date of Discharge  Length of Stay   Charges  Balance Remaining  \n",
      "0  Room_237        12/27/2023               0    237.34              37.34  \n",
      "1  Room_440         1/12/2024              22  29980.84            3018.84  \n",
      "2  Room_298        12/31/2023               6   4028.77            2681.26  \n",
      "3  Room_360        11/30/2023               0    273.69               0.00  \n",
      "4  Room_239          1/5/2024               1    490.50             341.28  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('docs/healthcare_dataset.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16048f4e-3fa8-4ffc-a012-ea80772f5687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 50\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(f\"Total number of rows: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bff09-ab2b-4fec-b409-c6a0894536d4",
   "metadata": {},
   "source": [
    "## merge pdf and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "943737aa-fc12-4004-b20a-36a433ad47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the PDF and CSV loaders into a single dataset\n",
    "merged_loader = MergedDataLoader(loaders=[pdf_dir_loader, patient_data_csv_loader])\n",
    "\n",
    "# Load all the merged documents\n",
    "merged_documents = merged_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc391d19-092d-4b6a-93a7-34027c8c1aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da8b7e9c-045b-474e-a72d-cda810298ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CSV file rows are broken down and made into one document per row\n",
    "### 230 PDf file documents, + 50 rows of CSV file = 280 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc442b-b5f3-4b5f-9e40-341eec7490b2",
   "metadata": {},
   "source": [
    "PDF loader seems to treat each page as separate entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8aecfb9-776e-476f-b020-e98c7ffb50c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'docs/skin_cancer_prevention_update.pdf', 'page': 4}, page_content='July 15, 2005 U Volume 72, Number 2 www.aafp.org/afp American Family Physician  273\\ndiagnosis. 25 Some dermatology experts also add E for \\nevolution or for elevation above skin level. In general, \\nbenign lesions are round and symmetric, while melano-\\nmas are asymmetric. Benign lesions usually have regu-\\nlar margins, while melanomas have irregular borders. \\nBenign lesions are uniform in color, while melanomas \\nare more heterogeneous, with colors ranging from tan \\nto brown and black, often with areas of red, white, or \\nblue. Finally, most benign lesions are smaller than 6 mm \\nin diameter, while melanomas often are larger than 6 \\nmm at the time of diagnosis (Figures 3 through 10) . The \\nABCD checklist is a sensitive diagnostic test (90 to 100 \\npercent, depending on whether a positive test is defined \\nas the presence of one, two, or three of the ABCDs), but \\nthe specificity is not well defined.26 Table 227 describes \\nthe histologic subtypes of cutaneous melanoma.\\nSuspicious lesions should undergo full-thickness \\nbiopsy into the underlying subcutaneous tissue. Exci-\\nsional biopsy with 1- to 2-mm borders is preferred. The \\nexcisional biopsy should be oriented with the definitive \\ntreatment in mind. Incisional or punch biopsies may \\nb e  p e r f o r m e d  i f  l e s i o n  s i z e  o r  l o c a t i o n  m a k e s  e x c i -\\nsional biopsy inappropriate or impractical. Incisional \\nor punch biopsies should include the area of the lesion \\nthat appears most suspicious.11,28 A negative incisional or \\npunch biopsy does not necessarily rule out melanoma in \\na highly suspicious lesion. Shave biopsies should never \\nbe used if melanoma is suspected because lesion thick-\\nness is vitally important in determining treatment and \\nprognosis.\\nTreatment\\nLOCAL THERAPY\\nAfter melanoma is confirmed, patients must undergo \\ncomplete excision of the tumor or tumor site. Surgi-\\ncal resection is curative for local disease. Randomized \\ncontrolled trials comparing narrow excision (1 to 2 cm) \\nwith wide excision (more than 3 cm) consistently have \\nfound equivalent rates of local recurrence and disease-\\nfree and overall survival.29,30 Wide excision margins are \\nno longer recommended. Although the exact margins \\nFigure 8. Superficial spreading melanoma on ear.\\nFigure 9a. Superficial spreading melanoma on chest.\\nFigure 9b. Close-up of superficial spreading melanoma on \\nchest.\\nFigure 10. Superficial spreading melanoma with regression.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_documents[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda77e48-52b5-47a7-9634-13014096f2e3",
   "metadata": {},
   "source": [
    "## Transform source format to include URL for pdf chunks in documents \n",
    "This assumes an nginx instance running and pointing to a mounted pdf directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b21b4714-1d50-442d-a7c9-0b376764b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDF documents: 230\n",
      "Total CSV rows: 50\n",
      "docs/mental_health_first_aid.pdf\n",
      "docs/mental_health_first_aid.pdf\n",
      "docs/skin_cancer_prevention_update.pdf\n",
      "docs/skin_cancer_prevention_update.pdf\n",
      "docs/skin_cancer_prevention_update.pdf\n"
     ]
    }
   ],
   "source": [
    "# Prepend URL prefix to the source in metadata\n",
    "\n",
    "pdf_count = 0\n",
    "csv_count = 0\n",
    "\n",
    "for doc in merged_documents:\n",
    "    if 'source' in doc.metadata:\n",
    "        # Remove the directory part from the source path\n",
    "        file_name = doc.metadata['source'].replace(pdf_directory + \"/docs\", \"\")\n",
    "        doc.metadata['source'] = url_prefix + file_name\n",
    "\n",
    "        # Count the number of PDF and CSV documents\n",
    "        if file_name.lower().endswith('.pdf'):\n",
    "            pdf_count += 1\n",
    "        elif file_name.lower().endswith('.csv'):\n",
    "            csv_count += 1\n",
    "\n",
    "# Print the total number of PDF and CSV documents\n",
    "print(f\"Total PDF documents: {pdf_count}\")\n",
    "print(f\"Total CSV rows: {csv_count}\")\n",
    "\n",
    "\n",
    "# Print the updated sources to verify\n",
    "for doc in merged_documents[:5]:  # Print first 5 for verification\n",
    "    print(doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fa53c-d634-42a0-aa3a-703af8ed096b",
   "metadata": {},
   "source": [
    "### Chunk and split documents\n",
    "\n",
    "Each document will be chunked and split along the chunk_size parameter.  The overlap parameter will ADD to the amount of characters, so 512 plus 256 overlap will equal a split size of around 800.  An overlap of zero will equal a split size of only the chunk value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52cb147e-2332-42b3-9ae9-0b873e606651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=512, chunk_overlap=64)\n",
    "doc_splits = text_splitter.split_documents(merged_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1eb94-f0aa-4a1b-ac79-4af35f21c3ac",
   "metadata": {},
   "source": [
    "## Prepare Elasticsearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8577cbb-dd76-431d-af61-d1d706693224",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"nvd_agentic_rag_updated_merged_chunked_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99253b-15f8-4a83-ba1a-82e02b441e72",
   "metadata": {},
   "source": [
    "### Delete and rebuild ES index\n",
    "\n",
    "NOTE:  If you don't delete the index and start embedding duplicates into an existing index, you will get extremely bad performance and errors due to mulitiple documents with the same content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c34a399-4ac5-4ab2-bb61-69e191f9bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.elasticsearch.apps.ai-ocp.emea.dsc.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.elasticsearch.apps.ai-ocp.emea.dsc.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'nvd_agentic_rag_updated_merged_chunked_index' deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if the index exists and delete it if it does\n",
    "if es_client.indices.exists(index=INDEX_NAME):\n",
    "    es_client.indices.delete(index=INDEX_NAME)\n",
    "    print(f\"Index '{INDEX_NAME}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' does not exist, will be created in the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73db8d-f62c-4609-8758-c906a121b188",
   "metadata": {},
   "source": [
    "### Initial embed documents into vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dfb4b48-437d-4f20-928b-51ce9ad171c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.elasticsearch.apps.ai-ocp.emea.dsc.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.elasticsearch.apps.ai-ocp.emea.dsc.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.elasticsearch.apps.ai-ocp.emea.dsc.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.elasticsearch.apps.ai-ocp.emea.dsc.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete:\n",
      "CPU times: user 469 ms, sys: 123 ms, total: 592 ms\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorstore = ElasticsearchStore.from_documents(\n",
    "    doc_splits,\n",
    "    embeddings,\n",
    "    index_name=INDEX_NAME,\n",
    "    es_connection=es_client,\n",
    ")\n",
    "\n",
    "print('\\n' + 'Time to complete:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3f4db-e530-4a8e-82e1-c4d09e3295f0",
   "metadata": {},
   "source": [
    "### Verify document structure in Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80be3e9c-3f7f-4033-b211-8a2779909c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 687 documents in the index 'nvd_agentic_rag_updated_merged_chunked_index'.\n",
      "Document ID: da81a066-df52-462e-a120-3adc470da95b\n",
      "Document structure: {'text': '133\\nMei\\xa0C, McGorry\\xa0PD. Evid Based Ment Health 2020;23:133–134. doi:10.1136/ebmental-2020-300154\\nPerspective\\nMental health first aid: strengthening its impact for \\naid\\xa0recipients\\nCristina Mei    ,1,2 Patrick D McGorry1,2\\nTo cite: Mei\\xa0C, McGorry\\xa0PD. \\nEvid Based Ment Health \\n2020;23:133–134.\\n1Orygen, Parkville, Victoria, \\nAustralia\\n2Centre for Youth Mental \\nHealth, University of Melbourne, \\nParkville, Victoria, Australia\\nCorrespondence to\\nProfessor Patrick D McGorry, \\nOrygen, Parkville, VIC 3052, \\nAustralia;  pat. mcgorry@ orygen. \\norg. au\\nReceived 6 April 2020\\nRevised 4 June 2020\\nAccepted 9 June 2020\\nPublished Online First \\n29\\xa0July\\xa02020\\n© Author(s) (or their \\nemployer(s)) 2020. No \\ncommercial re- use. See rights \\nand permissions. Published \\nby BMJ.\\nABSTRACT\\nMental Health First Aid (MHFA) is a potentially valuable \\nfirst response in mental healthcare. MHFA is formulated \\nas an extension of Psychological First Aid, the latter \\nbeing a more focal response to crises and disasters. \\nMHFA is a broader strategy which aims to improve \\nthe general public’s immediate response to mental ill \\nhealth and mental health crisis. While its effect on those \\ntrained in MHFA has been promising, recent meta- \\nanalyses have failed to detect any significant benefit to \\nindividuals who receive support from an MHFA trainee. \\nSuch outcomes highlight the need to revisit the content \\nand implementation of MHFA to optimise and realise \\nthe full potential of the concept. Possible solutions are \\ndiscussed, including developing new MHFA content using \\nmethodologies that foster innovation and creativity, in \\naddition to improving the quality and effectiveness of \\nMHFA training.\\nConsiderable progress has been achieved in \\nimproving public awareness of mental illness and \\nreducing its stigma. 1 However, this has not driven', 'metadata': {'source': 'docs/mental_health_first_aid.pdf', 'page': 0}, 'vector': [-0.003326416015625, -0.02520751953125, 0.03460693359375, -0.01381683349609375, 0.07659912109375, 0.01490020751953125, -0.00655364990234375, -0.044036865234375, -0.006778717041015625, -0.0207977294921875, -0.0161590576171875, 0.0030651092529296875, -0.02313232421875, -0.045318603515625, 0.01010894775390625, 0.004894256591796875, 0.08258056640625, -0.0199432373046875, 0.0241546630859375, 0.044830322265625, 0.0276336669921875, -0.001979827880859375, -0.0119171142578125, 0.037567138671875, 0.049530029296875, 0.0281524658203125, 0.03692626953125, 0.00624847412109375, -0.0027523040771484375, -0.00925445556640625, 0.0289306640625, 5.0127506256103516e-05, -0.01904296875, -0.036712646484375, 0.007152557373046875, -0.0264129638671875, -0.019775390625, -0.046295166015625, 0.01395416259765625, -0.0113525390625, 0.0289306640625, -0.0131072998046875, 0.0119781494140625, 0.03485107421875, -0.043670654296875, 0.0110321044921875, 0.002971649169921875, -0.0009937286376953125, -0.004268646240234375, -0.0031032562255859375, -0.0142974853515625, 0.02447509765625, -0.039764404296875, 0.02105712890625, -0.020263671875, -0.0465087890625, -0.0292510986328125, -0.002635955810546875, 0.0122528076171875, -0.027557373046875, -0.021484375, -0.0184478759765625, -0.043975830078125, -0.0207672119140625, 0.02392578125, 0.0379638671875, 0.06085205078125, 0.0236663818359375, -0.0064544677734375, -0.00933074951171875, 0.0216064453125, 0.0307464599609375, 0.0322265625, -0.01505279541015625, 0.032318115234375, -0.012420654296875, -0.045379638671875, 0.00397491455078125, 0.044830322265625, 0.043914794921875, 0.001007080078125, -0.036895751953125, 0.0073394775390625, 0.09283447265625, 0.0203094482421875, 0.018463134765625, 0.07464599609375, 0.0094451904296875, 0.01554107666015625, -0.06842041015625, -0.0236053466796875, -0.020660400390625, 0.01189422607421875, -0.01326751708984375, -0.0140533447265625, -0.00450897216796875, 0.00994873046875, 0.0203094482421875, 0.043609619140625, -0.0413818359375, -0.007228851318359375, -0.03680419921875, -0.005397796630859375, -0.006153106689453125, -0.0189666748046875, -0.03350830078125, -0.06646728515625, -0.04766845703125, 0.044097900390625, -0.0043487548828125, -0.0012969970703125, -0.0200958251953125, 0.0173187255859375, -0.0189666748046875, -0.017425537109375, -0.033660888671875, 0.023345947265625, 0.01087188720703125, 0.045623779296875, 0.0025386810302734375, 0.023956298828125, -0.038543701171875, 0.0037078857421875, -0.0203094482421875, -0.03924560546875, -0.04168701171875, -0.0162200927734375, 0.003917694091796875, -0.0196533203125, 0.006481170654296875, 0.0682373046875, 0.01512908935546875, 0.0232391357421875, 0.01517486572265625, -0.030548095703125, -0.00690460205078125, -0.030029296875, 0.034515380859375, 0.00457763671875, 0.03680419921875, -0.0162200927734375, 0.0211944580078125, -0.0259246826171875, 0.051788330078125, -0.01328277587890625, 0.040191650390625, -0.01438140869140625, -0.0213623046875, 0.0254669189453125, 0.0277099609375, -0.03985595703125, 0.02325439453125, -0.01556396484375, 0.0255584716796875, -0.038543701171875, -0.030029296875, 0.0082550048828125, 0.00269317626953125, 0.00389862060546875, 0.0258941650390625, 0.0794677734375, 0.017974853515625, 0.0004787445068359375, -0.005306243896484375, -0.045318603515625, 0.0266571044921875, 0.04376220703125, -0.026153564453125, 0.0272674560546875, 0.00937652587890625, 0.00405120849609375, -0.0162353515625, 0.0085906982421875, -0.0213623046875, -0.0139923095703125, -0.06683349609375, -0.012939453125, 0.000812530517578125, -0.033782958984375, -0.0309906005859375, 0.0052642822265625, -0.0380859375, -0.07012939453125, 0.0008130073547363281, 0.00521087646484375, 0.023956298828125, 0.03155517578125, -0.01519012451171875, -0.0257415771484375, -0.0382080078125, -0.03314208984375, -0.053863525390625, 0.0943603515625, -0.02386474609375, 0.038604736328125, 0.01763916015625, -0.0704345703125, 0.07171630859375, 0.006793975830078125, 0.07269287109375, 0.040313720703125, -0.00664520263671875, 0.029083251953125, -0.0081939697265625, 0.0047454833984375, -0.01348876953125, -0.020263671875, -0.01537322998046875, -0.0106201171875, -0.01503753662109375, -0.03485107421875, -0.0237274169921875, -0.021881103515625, 0.04266357421875, -0.0022373199462890625, -0.02606201171875, -0.0253753662109375, 0.00652313232421875, -0.027252197265625, -0.02423095703125, -0.0170440673828125, 0.0124664306640625, 0.0023651123046875, -0.026214599609375, 0.0226593017578125, 0.0408935546875, 0.0283203125, -0.045745849609375, 0.031585693359375, 0.001796722412109375, -0.03143310546875, -0.007175445556640625, -0.004573822021484375, -0.0158233642578125, -0.01053619384765625, 0.002124786376953125, 0.0122833251953125, 0.00020706653594970703, 0.01438140869140625, 0.00621795654296875, 0.01136016845703125, -0.023284912109375, 0.0190887451171875, 0.07318115234375, -0.0017518997192382812, 0.0255584716796875, -0.0136260986328125, -0.032928466796875, -0.030426025390625, 0.06787109375, 0.02191162109375, -0.015869140625, -0.050567626953125, 0.007904052734375, -0.04339599609375, 0.0294036865234375, 0.00896453857421875, -0.01788330078125, 0.022705078125, 0.0039825439453125, 0.036895751953125, -0.02392578125, -0.018157958984375, -0.06866455078125, -0.056304931640625, 0.0035343170166015625, -0.007724761962890625, 0.04052734375, 0.0024471282958984375, -0.0021533966064453125, -0.048004150390625, -0.0015230178833007812, -0.034149169921875, -0.02264404296875, -0.033111572265625, 0.007175445556640625, 0.0030574798583984375, 0.0278778076171875, -0.0004413127899169922, 0.0694580078125, -0.0073394775390625, 0.047088623046875, 0.0101776123046875, -0.06292724609375, -0.054107666015625, -0.0168304443359375, -0.047821044921875, 0.01453399658203125, 0.07037353515625, -0.01262664794921875, -0.016265869140625, 0.0016632080078125, 0.0489501953125, 0.02655029296875, 0.00298309326171875, -0.039947509765625, 0.02593994140625, 0.0064849853515625, -0.0277099609375, -0.01433563232421875, -0.001194000244140625, -0.04083251953125, -0.0028228759765625, 0.06488037109375, -0.0131072998046875, 0.005123138427734375, 0.051544189453125, 0.0163726806640625, 0.04425048828125, -0.01776123046875, -0.01904296875, 0.07513427734375, 0.00872802734375, -0.060638427734375, -0.0101776123046875, -0.0281524658203125, -0.0226898193359375, 0.058868408203125, 0.042327880859375, 0.0168914794921875, 0.02154541015625, 0.0114593505859375, 0.00940704345703125, 0.0199127197265625, -0.0139617919921875, 0.00970458984375, -0.01959228515625, -0.02392578125, 0.047454833984375, 0.0560302734375, 0.0017242431640625, -0.0308685302734375, 0.02044677734375, 0.01512908935546875, 0.0291900634765625, -0.042724609375, -0.0036754608154296875, 0.02557373046875, -0.005092620849609375, 0.007007598876953125, 0.048309326171875, -0.0029659271240234375, -0.0016841888427734375, -0.006816864013671875, -0.0279083251953125, 0.0160369873046875, 0.0294036865234375, 0.03131103515625, -0.00363922119140625, -0.050079345703125, -0.06219482421875, 0.01186370849609375, -0.06243896484375, -0.01751708984375, 0.040740966796875, 0.005931854248046875, 0.0291595458984375, -0.0204925537109375, 0.005886077880859375, 0.0489501953125, -0.0279541015625, 0.0171356201171875, 0.0190887451171875, -0.034149169921875, -0.0309295654296875, 0.0213470458984375, -0.00247955322265625, 0.029449462890625, 0.01323699951171875, 0.039093017578125, -0.002758026123046875, -0.031524658203125, -0.017608642578125, -0.01220703125, -0.01004791259765625, 0.0156097412109375, -0.01001739501953125, 0.033447265625, 0.0207977294921875, -0.01154327392578125, -0.012298583984375, -0.00502777099609375, 0.0082244873046875, 0.0185089111328125, -0.055267333984375, -0.00926971435546875, 0.01032257080078125, 0.01739501953125, 0.04901123046875, -0.0283050537109375, -0.0188446044921875, 0.041473388671875, 0.009246826171875, -0.072265625, 0.050567626953125, 0.06512451171875, -0.01110076904296875, -0.049224853515625, 0.0063934326171875, -0.0310821533203125, 0.0133514404296875, 0.01202392578125, 0.05462646484375, 0.0286102294921875, 0.012481689453125, -0.00429534912109375, -0.025177001953125, -0.0587158203125, -0.007259368896484375, 0.021484375, 0.01256561279296875, -0.005649566650390625, -0.0030918121337890625, -0.0701904296875, 0.0301055908203125, -0.013031005859375, 0.0025386810302734375, -0.04998779296875, 0.0174407958984375, 0.00861358642578125, -0.00995635986328125, -0.0517578125, 0.007373809814453125, -0.0308074951171875, -0.0006022453308105469, 0.0161285400390625, 0.0022792816162109375, -0.038909912109375, -0.02618408203125, 0.00806427001953125, 0.0034694671630859375, -0.04791259765625, 0.0112152099609375, -0.03887939453125, -0.0055999755859375, -0.032135009765625, -0.04071044921875, 0.0054779052734375, 0.01496124267578125, -0.0200958251953125, 0.019256591796875, 0.0198211669921875, 0.05694580078125, -0.050811767578125, 0.0133514404296875, 0.008636474609375, 0.033355712890625, 0.060211181640625, 0.04766845703125, 0.0587158203125, 0.0087432861328125, 0.0122833251953125, -0.020416259765625, 0.03466796875, -0.033477783203125, -0.033294677734375, 4.4465065002441406e-05, 0.07281494140625, -0.006877899169921875, 0.005039215087890625, -0.05322265625, 0.0004222393035888672, 0.05010986328125, -0.05877685546875, 0.003147125244140625, 0.037139892578125, 0.0246734619140625, 0.0199127197265625, -0.0180511474609375, -0.0614013671875, 0.042083740234375, -0.0014972686767578125, -0.01102447509765625, -2.1219253540039062e-05, 0.00661468505859375, 0.0136871337890625, 0.0142364501953125, -0.049560546875, 0.0181121826171875, 0.0002543926239013672, -0.0162200927734375, -0.0290374755859375, 0.0020275115966796875, -0.01904296875, -0.0997314453125, -0.02557373046875, 0.01013946533203125, -0.061553955078125, 0.0308685302734375, 0.0455322265625, -0.0250091552734375, 0.05487060546875, 0.0011196136474609375, -0.038177490234375, -0.002796173095703125, -0.047760009765625, 0.03070068359375, 0.035430908203125, 0.03314208984375, 0.0113525390625, 0.022491455078125, -0.0810546875, 0.01154327392578125, 0.0462646484375, -0.002780914306640625, -0.05474853515625, 0.0183563232421875, -0.0139923095703125, 0.03643798828125, -0.048583984375, 0.0113525390625, -0.004039764404296875, -0.010345458984375, 0.0308990478515625, 0.01126861572265625, 0.0180816650390625, 0.03631591796875, -0.05560302734375, 0.0014581680297851562, 0.0012865066528320312, -0.02655029296875, 0.00435638427734375, 0.047088623046875, 0.0282745361328125, -0.0205535888671875, -0.014801025390625, 0.018829345703125, -0.025848388671875, -0.00952911376953125, 0.005306243896484375, -0.0081634521484375, -0.0087432861328125, -0.010009765625, 0.039947509765625, 0.02392578125, -0.0308685302734375, -0.00514984130859375, 0.019256591796875, 0.00843048095703125, 0.09271240234375, 0.01300048828125, 0.005771636962890625, 0.020050048828125, -0.0160064697265625, -0.0003445148468017578, -0.017181396484375, 0.00034308433532714844, 0.015838623046875, 0.062164306640625, 0.0020885467529296875, -0.025970458984375, -0.021209716796875, -0.006755828857421875, 0.020782470703125, 0.0246124267578125, 0.024200439453125, 0.04315185546875, -0.04937744140625, 0.00653839111328125, 0.006343841552734375, -0.00958251953125, -0.0241241455078125, -0.045379638671875, -0.06219482421875, -0.0022907257080078125, -0.0003612041473388672, 0.06317138671875, -0.01438140869140625, -0.056549072265625, -0.0276031494140625, 0.01352691650390625, -0.042449951171875, 0.00827789306640625, 0.041961669921875, -0.0119781494140625, 0.0275421142578125, 0.01155853271484375, -0.00324249267578125, -0.01233673095703125, -0.047454833984375, -0.00839996337890625, -0.02392578125, -0.0509033203125, -0.0036163330078125, 0.08221435546875, -0.05511474609375, -0.005290985107421875, -0.015899658203125, 0.040283203125, -0.01477813720703125, -0.025054931640625, 0.0011157989501953125, -0.045074462890625, 0.02532958984375, 0.017181396484375, -0.03179931640625, 0.04974365234375, 0.0012722015380859375, -0.0084228515625, -0.00789642333984375, -0.0063629150390625, 0.01250457763671875, -0.0294342041015625, -0.006595611572265625, -0.0187530517578125, 0.0162200927734375, 0.044769287109375, 0.0085601806640625, 0.004932403564453125, -0.0190582275390625, -0.056396484375, -0.036865234375, 0.00569915771484375, 0.00957489013671875, 0.0025177001953125, 0.0296630859375, 0.0257415771484375, -0.041290283203125, 0.0136871337890625, 0.011260986328125, 0.0330810546875, 0.00506591796875, 0.01678466796875, 0.03436279296875, 0.0278778076171875, -0.1046142578125, 0.00998687744140625, -0.018096923828125, -0.05523681640625, 0.01108551025390625, -0.00222015380859375, -0.039215087890625, 0.038970947265625, -0.0007529258728027344, 0.017547607421875, -0.04168701171875, -0.0084991455078125, -0.035125732421875, -0.01435089111328125, 0.04608154296875, -0.03753662109375, 0.0308685302734375, -0.03143310546875, 0.036163330078125, -0.01161956787109375, 0.037139892578125, -0.02496337890625, -0.0259246826171875, -0.0452880859375, 0.0238189697265625, 0.003276824951171875, 0.01788330078125, -0.01018524169921875, -0.0013275146484375, 0.03924560546875, -0.045318603515625, 0.00537109375, -0.00746917724609375, 0.014190673828125, -0.0333251953125, 0.038909912109375, 0.033233642578125, -0.01015472412109375, 0.033477783203125, 0.0140533447265625, 0.1197509765625, 0.0131683349609375, 0.0132904052734375, -0.0357666015625, 0.0284271240234375, 0.0009059906005859375, 0.01849365234375, -0.003490447998046875, -0.01187896728515625, -0.0265045166015625, -0.0653076171875, 0.00020897388458251953, 0.0848388671875, -0.00010180473327636719, 0.004772186279296875, 0.021087646484375, -0.040618896484375, -0.031585693359375, -0.056854248046875, 0.004283905029296875, 0.0160675048828125, 0.0134124755859375, -0.0235595703125, -0.006870269775390625, -0.00063323974609375, -0.00856781005859375, 0.03533935546875, 0.01091766357421875, -0.004688262939453125, -0.0231781005859375, 0.01129913330078125, 0.01161956787109375, -0.056610107421875, -0.0094146728515625, 0.07635498046875, 0.02398681640625, 0.04541015625, -0.0274810791015625, -0.00499725341796875, -0.0042572021484375, -0.005184173583984375, 0.0013751983642578125, 0.0078887939453125, 0.0440673828125, 0.0089874267578125, -0.01233673095703125, -0.0272979736328125, 0.06109619140625, -0.00443267822265625, -0.0235595703125, 0.033721923828125, -0.01270294189453125, -0.033935546875, 0.0213623046875, -0.045867919921875, 0.048095703125, 0.0087738037109375, 0.0089874267578125, 0.0313720703125, 0.0182952880859375, 0.004642486572265625, 0.0017576217651367188, -0.01030731201171875, 0.00023126602172851562, -0.0205841064453125, -0.0830078125, -0.0038776397705078125, 0.023101806640625, 0.0277557373046875, 0.0252532958984375, -0.01690673828125, 0.0012273788452148438, -0.003162384033203125, 0.00147247314453125, 0.0043792724609375, -0.03753662109375, 0.01485443115234375, -0.006786346435546875, 0.0192718505859375, -0.039642333984375, -0.0038623809814453125, -0.015289306640625, -0.007122039794921875, 0.0170745849609375, 0.0261077880859375, 0.0277099609375, -0.0225982666015625, -0.005603790283203125, 0.0657958984375, 0.0211639404296875, 0.02557373046875, 0.044525146484375, -0.016510009765625, -0.0007562637329101562, -0.0330810546875, -0.00293731689453125, 0.0276947021484375, 0.01412200927734375, 0.04425048828125, -0.012786865234375, -0.016448974609375, 0.006183624267578125, -0.00572967529296875, -0.0238189697265625, 0.0209808349609375, 0.048828125, 0.0199432373046875, 0.004364013671875, -0.05120849609375, -4.881620407104492e-05, 0.0241851806640625, -0.03369140625, 0.042022705078125, -0.019317626953125, -0.005855560302734375, -0.0124664306640625, 0.0308685302734375, 0.042388916015625, -0.01255035400390625, 0.02252197265625, 0.01568603515625, -0.023193359375, -0.042816162109375, -0.0211029052734375, -0.0289306640625, -0.0227508544921875, -0.054107666015625, -0.0225372314453125, 0.013214111328125, 0.019775390625, 0.0276031494140625, 0.01552581787109375, -0.0125732421875, -0.033294677734375, 0.004016876220703125, 0.00389862060546875, 0.0044097900390625, -0.01039886474609375, -0.026702880859375, -0.01171112060546875, -0.028289794921875, 0.015960693359375, -0.0035152435302734375, 0.0189666748046875, 0.0275421142578125, 0.0063629150390625, -0.01241302490234375, 0.0509033203125, 0.00823211669921875, 0.054443359375, 0.0677490234375, 0.006282806396484375, -0.025604248046875, -0.040435791015625, -0.03302001953125, -0.0098114013671875, 0.0028972625732421875, -0.0164947509765625, -0.061492919921875, 0.032318115234375, -0.00742340087890625, 0.03765869140625, 0.004791259765625, 0.00634002685546875, 0.0184326171875, -0.01262664794921875, -0.0011110305786132812, -0.0109100341796875, 0.03875732421875, 0.006847381591796875, -0.002727508544921875, 0.001033782958984375, -0.02593994140625, 0.012481689453125, -0.044677734375, -0.003997802734375, 0.00963592529296875, -0.01438140869140625, 0.0306396484375, -0.00839996337890625, 0.0870361328125, -0.0380859375, 0.01226043701171875, 0.00787353515625, 0.0269775390625, 0.06427001953125, 0.0157012939453125, 0.1123046875, -0.030426025390625, 0.006908416748046875, -0.0297088623046875, 0.066162109375, -0.01446533203125, 0.019622802734375, 0.036285400390625, 0.0242767333984375, 0.0526123046875, 0.06109619140625, 0.0010080337524414062, -0.001800537109375, 0.07464599609375, -0.061737060546875, -0.0167236328125, -0.006191253662109375, -0.014190673828125, 0.0231781005859375, 0.02130126953125, 0.045196533203125, 0.0122833251953125, 0.02020263671875, -0.043304443359375, 0.01097869873046875, 0.046875, -0.034759521484375, -0.0184478759765625, 0.0107421875, -0.01146697998046875, 0.0137939453125, 0.02496337890625, 0.016204833984375, -0.05572509765625, 0.0194549560546875, 0.03228759765625, -0.06317138671875, -0.03057861328125, 0.0241546630859375, -0.006763458251953125, 0.0256195068359375, 0.0283966064453125, -0.083740234375, 0.03955078125, 0.0131072998046875, -0.01293182373046875, -0.0012378692626953125, 0.0292816162109375, -0.06768798828125, 0.04559326171875, 0.01611328125, 0.00994873046875, 0.016845703125, 0.001094818115234375, 0.0182952880859375, -0.0216522216796875, 0.0192108154296875, -0.01409912109375, 0.00215911865234375, 0.0149383544921875, 0.046112060546875, -0.03607177734375, 0.012664794921875, 0.0278778076171875, 0.0189971923828125, 0.024688720703125, -0.036590576171875, 0.0016460418701171875, 0.07061767578125, -0.039520263671875, -0.0005574226379394531, -0.01080322265625, 0.0296630859375, -0.0018558502197265625, 0.059417724609375, -0.04742431640625, -0.0025272369384765625, -0.01448822021484375, -0.004150390625, 0.0181121826171875, 0.032562255859375, -0.0013103485107421875, -0.0183868408203125, 0.0248260498046875, 0.0003323554992675781, -0.0158843994140625, 0.00926971435546875, -0.045989990234375, 0.0200958251953125, -0.0263824462890625, 0.037445068359375, -0.0089569091796875, 0.037628173828125, 0.06793212890625, -0.00036215782165527344, -0.06231689453125, -0.005168914794921875, 0.006221771240234375, -0.0236663818359375, -0.0303192138671875, -0.0167388916015625, 0.00504302978515625, 0.00041103363037109375, 0.058685302734375, 0.0285491943359375, 0.0087738037109375, -0.03564453125, 0.007289886474609375, -0.040191650390625, 0.0009026527404785156, -0.0082550048828125, -0.0276336669921875, 0.02215576171875, 0.0205078125, -0.018157958984375, -0.00957489013671875, 0.05035400390625, 0.0100860595703125, -0.045867919921875, -0.003749847412109375, -0.0240478515625, -0.066162109375, 0.00930023193359375, -0.00920867919921875, 0.01302337646484375, 0.014556884765625, -0.03289794921875, 0.043060302734375, 0.016326904296875, 0.00231170654296875, -0.030426025390625, -0.0188751220703125, -0.01082611083984375, 0.002567291259765625, -0.01898193359375, -0.0230865478515625, 0.0081939697265625, 0.01033782958984375, 0.011260986328125, 0.032989501953125, 0.0465087890625, 0.0158538818359375, -0.023681640625, 0.03466796875, 0.029876708984375, 0.09197998046875, -0.007740020751953125, -0.033935546875, 0.008056640625, -0.0478515625, 0.005596160888671875, 0.0386962890625, -0.00418853759765625, -0.0011730194091796875, -0.0445556640625, 0.06585693359375, -0.0577392578125, -0.003963470458984375, -0.048248291015625, -0.0196533203125, -0.0199127197265625, 0.0007371902465820312, -0.01239013671875, -0.0160064697265625, -0.007904052734375, 0.053558349609375, -0.0080718994140625, 0.00928497314453125, -0.001186370849609375, -0.01323699951171875, -0.041778564453125, -0.035003662109375]}\n",
      "--------------------------------------------------------------------------------\n",
      "Document ID: ef8ad278-e601-4b0d-a16b-18a1508fac77\n",
      "Document structure: {'text': 'methodologies that foster innovation and creativity, in \\naddition to improving the quality and effectiveness of \\nMHFA training.\\nConsiderable progress has been achieved in \\nimproving public awareness of mental illness and \\nreducing its stigma. 1 However, this has not driven \\nsubstantial changes in access to and quality of mental \\nhealthcare.2 Even in countries that have made \\nconsiderable gains in raising the public’s ability to \\nrecognise and respond to mental illness, challenges \\nremain in translating this into better outcomes for \\nthose experiencing mental illness. Mental Health \\nFirst Aid (MHFA) is a related strategy, which moves \\nbeyond awareness raising to a first response to \\npeople with mental ill health which has obvious \\nface validity. However, despite its wide dissem-\\nination and positive effect on MHFA trainees (ie, \\nindividuals who have completed MHFA training), \\nso far it has been difficult to demonstrate bene-\\nfits for the real targets of the programme, the end \\nusers or recipients of MHFA (ie, those people who \\nhave received support from a MHFA trainee). This \\nhas prompted a closer examination of the MHFA \\nprogramme to identify strategies for improvement.\\nMHFA is an attractive low- intensity first step in \\nsupporting people who are developing or experi-\\nencing mental health crises, offering a more prac-\\ntical approach which goes beyond merely raising \\nawareness. The concept of providing mental health- \\nrelated aid originated in the mid- 20th century \\nthrough the introduction of Psychological First \\nAid following disasters.3 Drawing on parallels with \\nmedical first aid, MHFA has usefully expanded the \\nconcept to encompass a broader range of mental \\ndisorders and crises. Using a ‘train- the- trainer’ \\nmodel it has spread rapidly across many parts of \\nthe world.\\nMHFA results in at least short- term benefits for \\nindividuals trained in MHFA. A recent meta- analysis \\nidentified improvements in knowledge about \\nmental health problems, beliefs about treatment, \\nidentification of mental health problems, intention \\nto provide MHFA, amount of help provided, and \\nconfidence in helping a person with a mental health \\nproblem, as well as reductions in stigmatising atti-', 'metadata': {'source': 'docs/mental_health_first_aid.pdf', 'page': 0}, 'vector': [0.0189361572265625, -0.0291748046875, -0.007167816162109375, -0.0096435546875, 0.0633544921875, -0.0012664794921875, -0.048004150390625, -0.03289794921875, 0.0169677734375, 0.01273345947265625, 0.00438690185546875, -0.0099029541015625, 0.005519866943359375, -0.057647705078125, 0.0019969940185546875, 0.040283203125, 0.06817626953125, -0.0007877349853515625, 0.010467529296875, -0.0254058837890625, 0.050750732421875, 0.00484466552734375, -0.02252197265625, 0.0360107421875, 0.043853759765625, 0.0200347900390625, 0.0301971435546875, -0.0074005126953125, -0.00247955322265625, -0.006587982177734375, 0.0185394287109375, -0.020538330078125, -0.0188446044921875, -0.030517578125, 0.0185394287109375, -0.0222625732421875, -0.013153076171875, -0.053436279296875, 0.007282257080078125, -0.0152587890625, 0.031402587890625, -0.0167083740234375, -0.0152740478515625, 0.007061004638671875, -0.044830322265625, -0.02587890625, 0.00261688232421875, -0.015869140625, -0.0546875, -0.0027484893798828125, -0.047698974609375, 0.0322265625, -0.0171356201171875, 0.04742431640625, -0.00911712646484375, -0.036285400390625, -0.002811431884765625, 0.01020050048828125, 0.039794921875, -0.0243682861328125, -0.030029296875, -0.038330078125, -0.05157470703125, -0.0224761962890625, 0.043121337890625, 0.035491943359375, 0.0236663818359375, 0.035369873046875, -0.036102294921875, -0.022186279296875, 0.0279083251953125, 0.0236663818359375, 0.0163421630859375, 0.0213775634765625, 0.044189453125, -0.01132965087890625, -0.00982666015625, 0.048431396484375, 0.064208984375, 0.05242919921875, 0.02459716796875, -0.05938720703125, 0.009918212890625, 0.08392333984375, 0.019805908203125, 0.0005130767822265625, 0.08343505859375, 0.0302886962890625, 0.044891357421875, -0.045135498046875, -0.0299072265625, -0.00661468505859375, 0.025909423828125, -0.0085906982421875, -0.01171112060546875, 0.01067352294921875, -0.0307769775390625, 0.02801513671875, 0.059112548828125, -0.050445556640625, 0.0045623779296875, -0.05206298828125, -0.0143890380859375, -0.049041748046875, -0.03717041015625, -0.043853759765625, -0.0699462890625, -0.0272674560546875, 0.033935546875, 0.0018405914306640625, -0.041595458984375, -0.024658203125, 0.0406494140625, -0.042938232421875, -0.00569915771484375, -0.01861572265625, -0.0015869140625, 0.0188751220703125, 0.03961181640625, -0.005130767822265625, 0.0546875, -0.03717041015625, 0.0202789306640625, -0.009002685546875, -0.04132080078125, -0.013916015625, -0.017974853515625, -0.03369140625, -0.0271759033203125, -0.006076812744140625, 0.045654296875, 0.002285003662109375, 0.0003979206085205078, 0.031494140625, -0.046966552734375, 0.046295166015625, -0.05078125, -0.01435089111328125, -0.00688934326171875, 0.048858642578125, -0.0034694671630859375, 0.01151275634765625, -0.0172271728515625, 0.0517578125, 0.0063018798828125, 0.0273895263671875, 0.01209259033203125, 0.0021953582763671875, 0.0179290771484375, -0.003192901611328125, -0.052886962890625, -0.017791748046875, -0.0254669189453125, 0.01446533203125, -0.013275146484375, -0.036346435546875, 0.0152435302734375, -0.0181427001953125, -0.01739501953125, 0.02410888671875, 0.072021484375, 0.032379150390625, 0.023681640625, -0.024139404296875, -0.0107879638671875, 0.046875, 0.061187744140625, -0.0210113525390625, 0.016021728515625, -0.005031585693359375, -0.01045989990234375, 0.008575439453125, 0.0180511474609375, -0.019256591796875, -0.00928497314453125, -0.05523681640625, 0.006011962890625, 0.01641845703125, -0.0229949951171875, -0.01299285888671875, 0.01605224609375, -0.0259552001953125, -0.047576904296875, -0.00490570068359375, -0.0096282958984375, 0.04248046875, 0.017791748046875, -0.01242828369140625, -0.019744873046875, -0.039306640625, -0.046661376953125, -0.04461669921875, 0.043212890625, -0.0259552001953125, 0.03961181640625, 0.003185272216796875, -0.06298828125, 0.09405517578125, 0.01267242431640625, 0.038482666015625, 0.022003173828125, -0.0282745361328125, 0.030303955078125, -0.0214691162109375, 0.0059661865234375, 0.015533447265625, 0.00482177734375, -0.0171356201171875, 0.0003254413604736328, -0.0286712646484375, -0.072509765625, -0.0027332305908203125, -0.019317626953125, 0.051727294921875, 0.013519287109375, 0.018218994140625, -0.05718994140625, 0.01204681396484375, -0.0802001953125, -0.00897216796875, -0.024169921875, 0.023162841796875, 0.0070953369140625, -0.038360595703125, 0.0521240234375, -0.00740814208984375, 0.01468658447265625, -0.03485107421875, 0.0161285400390625, -0.00972747802734375, -0.025604248046875, -0.0182647705078125, -0.00391387939453125, -0.0015869140625, -0.03228759765625, -0.01236724853515625, 0.01543426513671875, -0.009552001953125, 0.0460205078125, 0.0087738037109375, 0.001529693603515625, 0.0227813720703125, 0.026702880859375, 0.09881591796875, -0.004974365234375, 0.039581298828125, -0.00775909423828125, -0.0176239013671875, -0.025054931640625, 0.05029296875, 0.037261962890625, -0.0009546279907226562, -0.07257080078125, -0.0091552734375, -0.036346435546875, 0.041351318359375, -0.0232086181640625, -0.033416748046875, 0.03265380859375, 0.016357421875, 0.0347900390625, -0.01174163818359375, -0.0340576171875, -0.045806884765625, -0.0035648345947265625, 0.018829345703125, -0.00562286376953125, 0.0274200439453125, 0.0291748046875, -0.0080718994140625, -0.061187744140625, -0.03131103515625, -0.04931640625, -0.0170745849609375, 0.036376953125, -0.01160430908203125, -0.00238800048828125, 0.0306549072265625, 0.00011616945266723633, 0.04644775390625, -0.01067352294921875, 0.03009033203125, -0.01244354248046875, -0.0401611328125, -0.0231781005859375, -0.0266571044921875, -0.04425048828125, -0.01154327392578125, 0.07568359375, -0.02630615234375, -0.038116455078125, -0.03680419921875, -0.0020122528076171875, 0.047943115234375, 0.0175628662109375, -0.0180206298828125, 0.048980712890625, 0.01190948486328125, -0.007354736328125, 0.00800323486328125, 0.01922607421875, -0.022125244140625, 0.00983428955078125, 0.03570556640625, -0.026519775390625, -0.012939453125, 0.044219970703125, 0.0178070068359375, 0.036529541015625, -0.0092620849609375, -0.0482177734375, 0.053375244140625, 0.0019855499267578125, -0.042022705078125, -0.0199737548828125, -0.031036376953125, 0.00946044921875, 0.02105712890625, 0.058074951171875, 0.00748443603515625, 0.03228759765625, 0.01494598388671875, -0.0247650146484375, 0.0107879638671875, -0.0291900634765625, 0.005245208740234375, -0.019744873046875, -0.03155517578125, 0.0765380859375, 0.036865234375, 0.0005002021789550781, -0.0296478271484375, 0.0501708984375, 0.047119140625, 0.0195159912109375, -0.0308837890625, 0.01139068603515625, 0.016082763671875, -0.046173095703125, 0.0229034423828125, 0.047088623046875, -0.0147857666015625, -0.033935546875, -0.048370361328125, -0.021575927734375, 0.048583984375, -0.003543853759765625, 0.0330810546875, -0.027679443359375, -0.0163116455078125, -0.0259552001953125, 0.0293121337890625, -0.041778564453125, 0.0002892017364501953, 0.0111541748046875, -0.008453369140625, 0.03363037109375, -0.027099609375, 0.0110626220703125, 0.004741668701171875, -0.01338958740234375, 0.0180816650390625, 0.03863525390625, -0.031463623046875, -0.04736328125, -0.011322021484375, 0.01922607421875, 0.0110931396484375, 0.0157012939453125, 0.039306640625, 0.02752685546875, -0.04168701171875, -0.004558563232421875, -0.042694091796875, -0.0253753662109375, -0.01154327392578125, -0.01071929931640625, 0.052337646484375, 0.0175933837890625, -0.008880615234375, 0.0067596435546875, -0.014434814453125, 0.00812530517578125, 0.00861358642578125, -0.043121337890625, 0.005382537841796875, 0.0034084320068359375, 0.0231781005859375, 0.033935546875, -0.0291748046875, 0.001888275146484375, 0.03082275390625, -0.0128631591796875, -0.040679931640625, 0.040924072265625, 0.0273895263671875, 0.017852783203125, -0.036712646484375, -0.0167388916015625, -0.02783203125, 0.011077880859375, 0.033935546875, 0.06683349609375, 0.00887298583984375, 0.0031833648681640625, 0.01157379150390625, -0.0281219482421875, -0.0489501953125, -0.0307769775390625, 0.02264404296875, 0.020233154296875, 0.0030364990234375, 0.043212890625, -0.0673828125, 0.0247955322265625, 0.019256591796875, -0.0019292831420898438, -0.034515380859375, 0.0168609619140625, 0.0341796875, -0.019683837890625, -0.03570556640625, 0.01509857177734375, -0.046905517578125, 0.03729248046875, 0.0266571044921875, -0.0168914794921875, -0.018890380859375, -0.048187255859375, 0.02105712890625, 0.03887939453125, -0.04071044921875, 0.0027980804443359375, -0.0521240234375, -0.01271820068359375, -0.0252838134765625, -0.03594970703125, 0.00667572021484375, 0.023162841796875, -0.001617431640625, 0.0143280029296875, 0.057220458984375, 0.03179931640625, -0.05426025390625, -0.011260986328125, 0.016632080078125, 0.012115478515625, 0.06927490234375, 0.0253143310546875, 0.0298919677734375, -0.0017137527465820312, 0.033721923828125, -0.033721923828125, 0.01412200927734375, -0.0355224609375, -0.023681640625, -0.007427215576171875, 0.01497650146484375, -0.0191650390625, 0.0069580078125, -0.04425048828125, 0.0236663818359375, 0.03424072265625, -0.041351318359375, -0.019744873046875, 0.051727294921875, 0.0128326416015625, 0.0054168701171875, -0.043701171875, -0.055511474609375, 0.052642822265625, 0.017181396484375, -0.040771484375, 0.00434112548828125, -0.0158538818359375, 0.0183868408203125, 0.0262908935546875, -0.032623291015625, 0.053131103515625, -0.0011663436889648438, -0.0166473388671875, -0.00792694091796875, -0.01493072509765625, -0.02130126953125, -0.0704345703125, -0.042144775390625, -0.01482391357421875, -0.08319091796875, 0.0303497314453125, 0.00759124755859375, -0.035308837890625, 0.007305145263671875, -0.0097808837890625, -0.052886962890625, -0.0289306640625, -0.03857421875, 0.008270263671875, 0.0693359375, 0.0433349609375, 0.01995849609375, -0.0047760009765625, -0.07537841796875, 0.01378631591796875, 0.051727294921875, 9.709596633911133e-05, 0.00505828857421875, 0.032440185546875, -0.0052032470703125, 0.025390625, -0.004302978515625, 0.0026035308837890625, -0.003879547119140625, -0.033538818359375, -0.01049041748046875, 0.0013275146484375, 0.0022716522216796875, 0.043212890625, -0.08343505859375, -0.01471710205078125, 0.03485107421875, -0.0172271728515625, 0.025604248046875, 0.0044708251953125, 0.059906005859375, -0.01020050048828125, -0.0013036727905273438, 0.035675048828125, -0.0138397216796875, -0.015045166015625, -0.031646728515625, -0.0183258056640625, 0.02423095703125, -0.039306640625, 0.037139892578125, 0.0172119140625, -0.02825927734375, -0.004451751708984375, 0.023162841796875, -0.0003337860107421875, 0.036163330078125, 0.04461669921875, 0.01171112060546875, 0.0190277099609375, -0.025726318359375, 0.043243408203125, -0.04620361328125, -0.01027679443359375, 0.0277557373046875, 0.07171630859375, -0.008544921875, 0.00023603439331054688, -0.0028591156005859375, -0.03656005859375, 0.00696563720703125, 0.006717681884765625, -0.005329132080078125, 0.02557373046875, -0.03912353515625, 0.0225830078125, 4.744529724121094e-05, 0.006214141845703125, -0.0014562606811523438, 0.004421234130859375, -0.0287017822265625, 0.033294677734375, 0.0038604736328125, 0.06280517578125, -0.0272216796875, -0.044219970703125, -0.024566650390625, 0.0247344970703125, -0.043670654296875, 0.01059722900390625, 0.04034423828125, -0.0253448486328125, 0.01374053955078125, 0.0148468017578125, -0.00478363037109375, -0.0264434814453125, -0.051300048828125, -0.01477813720703125, -0.03802490234375, -0.024139404296875, 0.0175018310546875, 0.05316162109375, -0.01178741455078125, 0.0015897750854492188, 0.0005373954772949219, 0.01436614990234375, -0.051605224609375, -0.0029811859130859375, 0.00066375732421875, -0.06317138671875, 0.0362548828125, 0.04071044921875, -0.0279083251953125, 0.058837890625, -0.0011386871337890625, -0.0199737548828125, -0.042938232421875, 0.012176513671875, 0.03192138671875, -0.051300048828125, 0.0012645721435546875, -0.0196685791015625, 0.02587890625, 0.041412353515625, 0.0177154541015625, 0.02978515625, 0.006198883056640625, -0.0208892822265625, -0.0181427001953125, 0.040802001953125, -0.04644775390625, 0.025054931640625, 0.02099609375, 0.0296478271484375, 0.0013866424560546875, 0.0254058837890625, 0.049530029296875, 0.046142578125, 0.041900634765625, -0.00896453857421875, 0.08111572265625, 0.00988006591796875, -0.0869140625, 0.0146331787109375, -0.0112457275390625, -0.045806884765625, 0.03656005859375, -0.003147125244140625, -0.031768798828125, 0.0156707763671875, -0.0164031982421875, -0.0034008026123046875, -0.032745361328125, 0.0162811279296875, -0.0208892822265625, 0.0242156982421875, 0.030914306640625, -0.007556915283203125, 0.0311126708984375, -0.057342529296875, -0.0028133392333984375, 0.0287322998046875, 0.02587890625, -0.006282806396484375, -0.01995849609375, -0.0263214111328125, 0.03436279296875, -0.0024852752685546875, 0.033416748046875, -0.0046844482421875, -0.0233001708984375, 0.022216796875, -0.06658935546875, 0.013671875, 0.00495147705078125, 0.01316070556640625, -0.051055908203125, 0.00438690185546875, 0.0190887451171875, -0.02947998046875, 0.06549072265625, 0.029693603515625, 0.1414794921875, 0.022369384765625, 0.0241851806640625, -0.04559326171875, 0.00814056396484375, -0.0158233642578125, 0.0006194114685058594, -0.007717132568359375, 0.02166748046875, -0.0174713134765625, -0.0439453125, 0.01238250732421875, 0.039306640625, 0.018524169921875, 0.0214996337890625, 0.037506103515625, -0.032257080078125, -0.04034423828125, -0.01456451416015625, -0.00768280029296875, 0.0291748046875, 0.0087432861328125, -0.00788116455078125, -0.01971435546875, 0.0038585662841796875, 0.0232696533203125, 0.05401611328125, 0.006587982177734375, -0.00662994384765625, -0.0021877288818359375, -0.005847930908203125, 0.03277587890625, -0.02850341796875, -0.006134033203125, 0.0816650390625, 0.01131439208984375, 0.0161285400390625, -0.0200653076171875, -0.0137786865234375, 0.02508544921875, 0.0171051025390625, -0.004856109619140625, -0.033477783203125, 0.0245361328125, -0.0177459716796875, -0.009765625, -0.0158538818359375, 0.0218658447265625, -0.0297698974609375, 0.00640106201171875, 0.00283050537109375, -0.06109619140625, -0.0214385986328125, 0.0233612060546875, -0.0283966064453125, 0.0231475830078125, 0.0168609619140625, -0.0151824951171875, 0.03704833984375, 0.019866943359375, 0.038177490234375, -0.0299072265625, -0.003997802734375, 0.0022602081298828125, -0.015838623046875, -0.07623291015625, -0.0010223388671875, 0.0195465087890625, 0.015106201171875, 0.025177001953125, -0.04046630859375, -0.00890350341796875, 0.0015459060668945312, -0.005458831787109375, 0.019378662109375, -0.01195526123046875, 0.0271759033203125, 0.037750244140625, -0.00634765625, -0.058685302734375, -0.01273345947265625, -0.047637939453125, 0.028778076171875, 0.0257415771484375, 0.03179931640625, -0.0010280609130859375, -0.01128387451171875, -0.009185791015625, 0.0300750732421875, -0.0010023117065429688, -0.004795074462890625, 0.0394287109375, 0.013031005859375, 0.01459503173828125, -0.028778076171875, 0.00809478759765625, 0.01256561279296875, -0.0175933837890625, 0.0193939208984375, 0.01396942138671875, -0.016357421875, -0.0146331787109375, 0.00579071044921875, -0.0111541748046875, 0.027099609375, 0.07568359375, 0.032196044921875, -0.03289794921875, -0.0307464599609375, -0.01485443115234375, 0.054473876953125, -0.019622802734375, 0.035858154296875, -0.0270843505859375, -0.048583984375, 0.007572174072265625, 0.0293731689453125, 0.0071258544921875, 0.0022945404052734375, 0.0322265625, 0.040191650390625, -0.024627685546875, -0.0187835693359375, -0.0135650634765625, -0.0271759033203125, -0.025482177734375, -0.0289154052734375, -0.01111602783203125, 0.029083251953125, -0.01485443115234375, 0.056793212890625, 0.002864837646484375, 0.0102996826171875, -0.0250091552734375, 0.01702880859375, -0.015899658203125, 0.0094146728515625, -0.0322265625, -0.01209259033203125, -0.0174560546875, 0.0089263916015625, -0.004367828369140625, 0.0029087066650390625, -0.02630615234375, 0.071044921875, 0.047210693359375, -0.0128936767578125, 0.037933349609375, 0.0218963623046875, 0.026397705078125, 0.0157928466796875, -0.050872802734375, 0.0007815361022949219, -0.049041748046875, -0.0290374755859375, -0.0249481201171875, 0.013763427734375, -0.033782958984375, -0.03521728515625, -0.0011806488037109375, 0.0010538101196289062, 0.0300445556640625, 0.012054443359375, 0.006988525390625, 0.026123046875, -0.00397491455078125, -0.00469970703125, 0.0091400146484375, 0.04522705078125, 0.01611328125, 0.011077880859375, -0.0127410888671875, -0.061614990234375, 0.0433349609375, -0.04931640625, -0.00531005859375, 0.02850341796875, -0.0267181396484375, 0.0032482147216796875, -0.006679534912109375, 0.08966064453125, -0.03289794921875, 0.002124786376953125, 0.02960205078125, -0.0017976760864257812, 0.038116455078125, 0.00159454345703125, 0.09161376953125, -0.020050048828125, 0.021270751953125, -0.02166748046875, 0.054534912109375, 0.0244293212890625, -0.01149749755859375, 0.036895751953125, -0.0044708251953125, 0.0196533203125, 0.0248260498046875, 0.0208740234375, -0.0423583984375, 0.049072265625, -0.0183868408203125, -0.01477813720703125, 0.0189971923828125, -0.045654296875, 0.02294921875, 0.0298614501953125, 0.040313720703125, 0.026214599609375, 0.00853729248046875, -0.0230712890625, -0.035614013671875, 0.02374267578125, -0.0156097412109375, -0.0091400146484375, 0.01971435546875, -0.00750732421875, -0.032958984375, 0.0159454345703125, 0.0270843505859375, -0.060150146484375, 0.048797607421875, 0.01197052001953125, -0.06756591796875, -0.0233306884765625, 0.00370025634765625, 0.0199737548828125, 0.0211181640625, 0.026092529296875, -0.04107666015625, 0.0648193359375, 0.019622802734375, 0.01363372802734375, -0.0006427764892578125, 0.01555633544921875, -0.039031982421875, 0.0200042724609375, 0.006450653076171875, 0.003467559814453125, -0.007843017578125, -0.035980224609375, 0.000514984130859375, 0.015167236328125, 0.0293121337890625, -0.0189971923828125, 0.0040130615234375, 0.00156402587890625, 0.0677490234375, -0.0543212890625, 0.045928955078125, 0.032806396484375, 0.0254364013671875, -0.0267791748046875, -0.033477783203125, 0.0303497314453125, 0.034698486328125, -0.02978515625, -0.0209197998046875, -0.006793975830078125, -0.0201568603515625, -0.004322052001953125, 0.06292724609375, -0.022613525390625, -0.007205963134765625, -0.00946807861328125, 0.0197906494140625, 0.031463623046875, 0.031646728515625, 0.01477813720703125, -0.059783935546875, 0.0029811859130859375, -0.0179443359375, -0.027099609375, 0.01073455810546875, -0.01302337646484375, 0.00757598876953125, -0.0203399658203125, 0.02734375, -0.0031719207763671875, 0.01529693603515625, 0.046722412109375, 0.034942626953125, -0.03125, -0.019927978515625, -0.00801849365234375, -0.02752685546875, -0.0219879150390625, -0.0032672882080078125, 0.00279998779296875, 0.055908203125, 0.03851318359375, -0.0030384063720703125, 0.0233612060546875, -0.0285186767578125, 0.008056640625, -0.02276611328125, -0.0266571044921875, -0.0233612060546875, -0.03759765625, 0.006229400634765625, 0.043731689453125, 0.013214111328125, -0.0263214111328125, 0.06005859375, -0.017425537109375, -0.0235443115234375, 0.043243408203125, -0.017974853515625, -0.00804901123046875, 0.0164947509765625, -0.0291290283203125, 0.015655517578125, 0.0157318115234375, -0.04583740234375, -0.000507354736328125, -0.00284576416015625, -0.00830078125, -0.0243682861328125, 0.031524658203125, 0.016448974609375, 0.006450653076171875, -0.004131317138671875, -0.026397705078125, -0.00592041015625, 0.01678466796875, 0.03485107421875, 0.010040283203125, 0.060333251953125, -0.02374267578125, -0.031890869140625, -0.018402099609375, 0.027099609375, 0.0863037109375, -0.0484619140625, -0.03228759765625, 0.034149169921875, -0.041900634765625, 0.0084228515625, 0.036224365234375, -0.0279388427734375, -0.019775390625, -0.012054443359375, 0.01776123046875, -0.02618408203125, -0.022186279296875, -0.03656005859375, -0.0145111083984375, 0.01080322265625, 0.029815673828125, -0.00978851318359375, -0.03656005859375, -0.01837158203125, 0.035980224609375, -0.02691650390625, -0.00021910667419433594, -0.00217437744140625, -0.0263824462890625, -0.034271240234375, -0.04742431640625]}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.elasticsearch.apps.ai-ocp.emea.dsc.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to check the structure of documents in the index\n",
    "def check_document_structure(index_name, es_client, num_docs=2):\n",
    "    # Search for documents in the index\n",
    "    response = es_client.search(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "            },\n",
    "            \"size\": num_docs\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Check if documents are found\n",
    "    if response['hits']['total']['value'] > 0:\n",
    "        print(f\"Found {response['hits']['total']['value']} documents in the index '{index_name}'.\")\n",
    "        for doc in response['hits']['hits']:\n",
    "            print(f\"Document ID: {doc['_id']}\")\n",
    "            print(f\"Document structure: {doc['_source']}\")\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        print(f\"No documents found in the index '{index_name}'.\")\n",
    "\n",
    "# Check the structure of documents\n",
    "check_document_structure(INDEX_NAME, es_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7244c-b979-480e-9f24-38a89ddd3a64",
   "metadata": {},
   "source": [
    "### Create direct vectorstore retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12287fcf-e8d9-4b8f-a445-8792d1b63ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e2c61fa-54ef-4eec-81e9-1da7c1cd9d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of unique files in merged loader, sorted by file type:\n",
      "\n",
      "docs/healthcare_dataset.csv\n",
      "docs/ipilimumab-for-advanced-melanoma-a-pharmacologic-perspective.pdf\n",
      "docs/covid_predictor_in_older_patients.pdf\n",
      "docs/skin_cancer_screening.pdf\n",
      "docs/covid_variants.pdf\n",
      "docs/covid_depression_in_72_yr_old_case_study.pdf\n",
      "docs/mental_health_first_aid.pdf\n",
      "docs/covid_update.pdf\n",
      "docs/population_based_approach_to_mental_health.pdf\n",
      "docs/understanding-pharmacology-covid-mrna.pdf\n",
      "docs/skin_cancer_prevention_update.pdf\n",
      "docs/investigating-the-efficacy-of-osimertinib-and-crizotinib-in-phase-3-clinical-trials-on-anti-cancer.pdf\n",
      "docs/skin_cancer_cells.pdf\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def get_unique_files(merged_documents):\n",
    "    file_list = []\n",
    "    \n",
    "    for doc in merged_documents:\n",
    "        source = doc.metadata.get('source', None)\n",
    "        if source:\n",
    "            file_list.append(source)\n",
    "    \n",
    "    # Use a set to get unique file URLs\n",
    "    unique_list = list(set(file_list))\n",
    "    \n",
    "    # Sort the unique list by file extension\n",
    "    sorted_unique_list = sorted(unique_list, key=lambda x: x.split('.')[-1])\n",
    "    \n",
    "    print(\"\\nList of unique files in merged loader, sorted by file type:\\n\")\n",
    "    for unique_file in sorted_unique_list:\n",
    "        print(unique_file)\n",
    "    \n",
    "    pretty_files = json.dumps(sorted_unique_list, indent=4, default=str)\n",
    "    \n",
    "    return pretty_files\n",
    "\n",
    "# Example usage\n",
    "unique_files_sorted = get_unique_files(merged_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d438f1-9de0-4531-825d-c8a008f3119b",
   "metadata": {},
   "source": [
    "# Setup and Test Agent pipeline elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39d22c-a5f1-4f89-b053-48b22483e8c8",
   "metadata": {},
   "source": [
    "### Generate RAG Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d531a81-6d4d-405e-975a-01ef1c9679fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "llm_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant in a health care clinic. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Chain\n",
    "llm_chain = llm_prompt | llm | StrOutputParser()\n",
    "\n",
    "# test\n",
    "# question = \"Tell me about mental health from a population perspective.\"\n",
    "# generation = llm_chain.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "372c83d1-47ad-4ab0-b1d3-61f061d44b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2a38d-5526-40c2-9618-465a710d0d98",
   "metadata": {},
   "source": [
    "### LLM-only toggle function\r\n",
    "\r\n",
    "Send request only to LLM directly, bypass RAG vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84accb68-ce9a-4783-94e3-3983e084175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(question):\n",
    "    generation = llm_chain.invoke({\"question\": question})\n",
    "    \n",
    "    return generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1df16-5d9d-4480-a50e-eff9e1784d70",
   "metadata": {},
   "source": [
    "### RAG chain setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20734917-1f1f-4546-9347-63528e0fb89c",
   "metadata": {},
   "source": [
    "## https://medium.com/@callumjmac/implementing-rag-in-langchain-with-chroma-a-step-by-step-guide-16fc21815339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c7f9cb9-30dc-4fa4-a139-d3fe8f5cd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant in a health care clinic. \n",
    "    Use the following pieces of retrieved context to answer the question first. Always ensure you directly address the user's question explicitly and focus on providing a clear and accurate answer. \n",
    "    After answering the question, provide actionable next steps as the final part of your response. Ensure the next steps are practical, relevant, and tailored to the context provided. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Keep your responses concise, actionable, and tailored to the context provided. \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer and Next Steps: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# # Query vector db and get similarity distance scores\n",
    "\n",
    "# question = \"Tell me about mental health from a population perspective.\"\n",
    "\n",
    "# search_results = vectorstore.similarity_search_with_relevance_scores(question, k=5)\n",
    "\n",
    "# print(search_results)\n",
    "\n",
    "# documents = [Document(page_content=str(result[0].page_content), metadata={**result[0].metadata, \"score\": result[1]}) for result in search_results]\n",
    "\n",
    "# print(documents)\n",
    "\n",
    "# # # Print the scores and rankings\n",
    "# # for rank, (doc, vector_score) in enumerate(documents, start=1):\n",
    "# #     print(f\"-- --\\n\\nRank: {rank}, Score: {vector_score}, Document: {doc.page_content}\")\n",
    "\n",
    "# # # Generate the response\n",
    "# # generation = rag_chain.invoke({\"context\": [doc.page_content for doc, _ in results], \"question\": question})\n",
    "\n",
    "# for rank, doc in enumerate(documents, start=1):\n",
    "#     print(f\"-- --\\n\\nRank: {rank}, Score: {doc.metadata['score']}, Page: {doc.metadata['page']}, Source: {doc.metadata['source']}, Document: {doc.page_content}\")\n",
    "\n",
    "# # Generate the response\n",
    "# generation = rag_chain.invoke({\"context\": [doc.page_content for doc in documents], \"question\": question})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61db24-79d2-4bcf-96a1-ae5f7a9f18f9",
   "metadata": {},
   "source": [
    "### Question Router Chain setup\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9c910c1-738c-4bf7-bf9e-801862b227eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to a vectorstore or web search. Use the vectorstore exclusively for questions related to patient data, skin cancer, covid, or mental health. \n",
    "    You do not need to be stringent with keywords in the question related to these topics. If **any document** is found to be relevant in the vectorstore, stop immediately and generate the answer using that data. \n",
    "    **Do not perform a web search** if even one relevant document is found, regardless of the overall assessment of other documents.\n",
    "    If **no relevant data** is found at all in the vectorstore, or if the question is unrelated to these topics, use web_search.\n",
    "    \n",
    "    Provide the answer in JSON format with a single key called 'datasource' and a single answer either 'vectorstore' or 'websearch' as the value.\n",
    "    Please do not include a preamble or explanation. Your response should be formatted as follows: \\'{{\"datasource\": \"value\"}}\\'.\n",
    "\n",
    "    Example 1: A question that is not related to patient data, skin cancer, covid, or mental health should return with a response to use the web_search like this: \\'{{\"datasource\": \"websearch\"}}\\'.\n",
    "\n",
    "    Example 2: A question that is related to patient data, skin cancer, covid, or mental health and any relevant data is found in the vectorstore should return a response like this: \\'{{\"datasource\": \"vectorstore\"}}\\'.\n",
    "\n",
    "    Question to route: {question}\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = router_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# question = \"Tell me about mental health from a population perspective.\"\n",
    "\n",
    "# docs = retriever.invoke(question)\n",
    "\n",
    "# doc_txt = docs[1].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "375d0cac-a460-425f-ab14-6f4e45dacb59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(question_router\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mquestion\u001b[49m}))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'question' is not defined"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0d21f-907d-4ab7-9238-bce2437bfa9d",
   "metadata": {},
   "source": [
    "### Relevance / Retrieval Grader\n",
    "\n",
    "Checks index of vectorstore to see if there are relavent docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b008df98-8394-49da-8fb8-aefe2c90d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "retrieval_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'relevance_yes_no_score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = retrieval_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# question = \"Tell me about mental health from a population perspective.\"\n",
    "\n",
    "# docs = retriever.invoke(question)\n",
    "\n",
    "# doc_txt = docs[1].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b35cf00c-0e78-4f83-93cc-c5f31f0ede99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7446f-1aff-4081-9713-1bc9546f3164",
   "metadata": {},
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "Checks to see if the generation is grounded in truth using the source documents as a reference.  \n",
    "If the generation is grounded in truth, then the hallucination grader responds positively with Yes.\n",
    "\n",
    "If the generation is NOT grounded in truth and has no relavence with the source documents, the grader responds negatively with No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0261a9a4-de13-4dd8-b082-95305a3e43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score in JSON format with a \n",
    "    single key 'score' and no preamble or explanation, like this \\'{{\\'\"score\": \"yes\"\\'{{\\' or \\'{{\\'\"score\": \"no\"\\'{{\\'. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_grader_prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5e8510b-5754-4b80-b24a-77fb29d8aa20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d1d7b-a23d-4dac-94e2-76960192372d",
   "metadata": {},
   "source": [
    "### Answer Grader\n",
    "\n",
    "Is the answer provided \"useful\" to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df9f6944-4fee-4971-b3a7-2b81b44ed433",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score in JSON format with a \n",
    "    single key 'score' and no preamble or explanation, like this \\'{{\\'\"score\": \"yes\"\\'{{\\' or \\'{{\\'\"score\": \"no\"\\'{{\\'. \n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = answer_grader_prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48e35f06-95af-47ac-907e-66637b23ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43a82d-9718-4c64-8460-00ba3bd0f59f",
   "metadata": {},
   "source": [
    "### Web Search\n",
    "\n",
    "uses the python library for Tavily open search.  Create an account and API here:\n",
    "https://blog.tavily.com/getting-started-with-the-tavily-search-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edb4e751-9e81-4233-8b94-b066dbd838c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "023ff2db-eb4e-4d44-904c-ea061abc16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e023ee-4f63-4ade-8b47-aebe394e89f6",
   "metadata": {},
   "source": [
    "### Graph relations function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a142182-af08-4dea-9245-2922d7c37b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "################################ State ##############################\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add internet search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ccf64-be81-4eb8-ba74-2c9e1fbf345b",
   "metadata": {},
   "source": [
    "### Question router function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26d75d48-fa85-4d85-849a-b6efddb67761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "\n",
    "    \n",
    "    # Status message\n",
    "    global router_status, router_choice, routing_agent_panel_content\n",
    "    \n",
    "    target_source = question_router.invoke({\"question\": question})\n",
    "    \n",
    "    print(target_source)\n",
    "\n",
    "    # Initialize colors for routing panel in UI\n",
    "    web_color = \"white\"\n",
    "    rag_color = \"white\"\n",
    "\n",
    "    # Check if source is a dictionary\n",
    "    if isinstance(target_source, dict):\n",
    "        if \"datasource\" in target_source:\n",
    "            print(target_source[\"datasource\"])\n",
    "            router_choice = target_source[\"datasource\"]\n",
    "            \n",
    "            if target_source[\"datasource\"] == \"websearch\":\n",
    "                print(\"---DECISION: ROUTE QUESTION TO WEB SEARCH---\")\n",
    "                router_status = \"success\"\n",
    "                web_color = \"#4CBB17\"\n",
    "                rag_color = \"white\"\n",
    "                \n",
    "            elif target_source[\"datasource\"] == \"vectorstore\":\n",
    "                print(\"---DECISION: ROUTE QUESTION TO RAG---\")\n",
    "                router_status = \"success\"\n",
    "                web_color = \"white\"\n",
    "                rag_color = \"#4CBB17\"\n",
    "                \n",
    "        else:\n",
    "            print(\"Error: 'datasource' key not found in source\")\n",
    "    else:\n",
    "        print(\"Error: source is not a dictionary\")\n",
    "\n",
    "    \n",
    "    # HTML table generation with green-colored cell for the routed choice\n",
    "    # print(rag_color)\n",
    "    # print(web_color)\n",
    "\n",
    "    routing_agent_panel_content = f\"\"\"\n",
    "    <table style=\"width: 100%;\">\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td style=\"padding: 1; width: 50%; background-color: {web_color}; text-align: center;\"><b>Web Search</b></td>\n",
    "                <td style=\"padding: 1; width: 50%; background-color: {rag_color}; text-align: center;\"><b>RAG database</b></td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "\n",
    "    ### finish and return the results\n",
    "    \n",
    "    if router_choice == \"websearch\":\n",
    "        return \"websearch\"\n",
    "    elif router_choice == \"vectorstore\":\n",
    "        return \"vectorstore\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8e91d-acc2-43a1-b3af-9c0f8b889d53",
   "metadata": {},
   "source": [
    "### Retrieval function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9af52c28-a040-40fe-b61c-f4865b3464df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE USING NVIDIA EMBEDDINGS NIM---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "\n",
    "    ### set nearest neighbors here, it will be used in other function as well\n",
    "    \n",
    "    global k_nearest\n",
    "    \n",
    "    k_nearest = 5\n",
    "    \n",
    "    # Retrieval now with vector similarity score\n",
    "    search_results = vectorstore.similarity_search_with_score(question, k_nearest)\n",
    "\n",
    "\n",
    "    documents = [Document(page_content=str(result[0].page_content), metadata={**result[0].metadata, \"score\": result[1]}) for result in search_results]\n",
    "    \n",
    "    # print(documents)\n",
    "    \n",
    "    # Status message\n",
    "    global retrieve_status\n",
    "    \n",
    "    retrieve_status = \"success\"\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2fc8ad-0844-4bcc-ad3d-e85b85a5f1f4",
   "metadata": {},
   "source": [
    "### Rerank function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56743880-d070-4ce1-9587-d841bce6cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---NVIDIA RERANK NIM PROCESS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "#### reranking NIM will create and reformat the metatdata with a new relevance_score key value, the docs will reflect this new metadata\n",
    "    \n",
    "    # Reranking\n",
    "    documents = reranker.compress_documents(query=question, documents=documents)\n",
    "\n",
    "    # print(documents)\n",
    "\n",
    "    # Status message\n",
    "    global rerank_status\n",
    "    rerank_status = \"success\"\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482eac8-ee32-4cd8-8364-e67699666ea3",
   "metadata": {},
   "source": [
    "### Grade document relevance function setup\n",
    "This will grade and create a list of relevant and not relevant docs as well as the count of those docs. This will print to raw output but will also be available for use in other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6beee6e9-8712-4c62-900e-d5e1672ead7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "    If no document is relevant, we will set a flag to run web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    \n",
    "    global filtered_docs  # Declare the global variable to place docs in to be accessed in other functions\n",
    "    global not_relevant_docs  # Declare the global variable to place not relevant docs\n",
    "    global not_relevant_count  # Declare the global variable to place not relevant docs\n",
    "    global relevant_count  # Add this line\n",
    "\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    not_relevant_docs = []\n",
    "\n",
    "    web_search = \"Yes\"  # Default to Yes in case there is no relevant doc\n",
    "\n",
    "    ## take the page content value of documents retrieved and grade it against the question,\n",
    "    ## this means the filtered_docs array will only contain page content values, not file names\n",
    "\n",
    "    # Counter for not relevant documents\n",
    "    not_relevant_count = 0\n",
    "    relevant_count = 0  # Initialize relevant_count\n",
    "\n",
    "    \n",
    "### CATEGORIZE RELEVANT VS NOT RELEVANT DOCS\n",
    "\n",
    "\n",
    "    \n",
    "    for doc in documents:\n",
    "        relevance_yes_no_score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": doc.page_content}\n",
    "        )\n",
    "        grade = relevance_yes_no_score[\"relevance_yes_no_score\"]\n",
    "        \n",
    "        print(grade)\n",
    "\n",
    "        # Extract the source from metadata\n",
    "        source = doc.metadata.get('source', 'Unknown Source')\n",
    "        \n",
    "        # Extract a snippet of the page content\n",
    "        snippet = doc.page_content[:200]  # Adjust the number of characters as needed\n",
    "\n",
    "        # # Extract the similarity score - a score of the distance between 2 vectors, lower number is best\n",
    "        # similarity_score = doc.metadata.get('score', 'Unknown Score')\n",
    "\n",
    "        # # Extract the reranker relevance score - the higher the score is best\n",
    "        # rerank_relevance_score = doc.metadata.get('relevance_score', 'Unknown Score')\n",
    "\n",
    "        \n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":   ### set to lower case \n",
    "            # print(f\"---GRADE: DOCUMENT RELEVANT---\\nSource: {source}\\nSnippet: {snippet}\\nVector Distance Score: {similarity_score}\\nRerank Relevance Score: {rerank_relevance_score}\")\n",
    "            filtered_docs.append(doc)\n",
    "            # Since we found at least one relevant document, set web_search to \"No\"\n",
    "            web_search = \"No\"\n",
    "            relevant_count += 1  # Increment the relevant counter\n",
    "            \n",
    "            \n",
    "        # Document not relevant\n",
    "        else:\n",
    "            # print(f\"---GRADE: DOCUMENT NOT RELEVANT---\\nSource: {source}\\nSnippet: {snippet}\\nVector Distance Score: {similarity_score}\\nRerank Relevance Score: {rerank_relevance_score}\")\n",
    "            not_relevant_docs.append(doc)\n",
    "            not_relevant_count += 1  # Increment the counter\n",
    "            # Do not include the document in filtered_docs\n",
    "            # will default to web search = yes\n",
    "            continue\n",
    "\n",
    " \n",
    "    # Status message\n",
    "    global relevance_status\n",
    "    relevance_status = \"success\"\n",
    "\n",
    "    global relevance_report_msg\n",
    "    \n",
    "    # Check if relevant documents are less than half of the total documents\n",
    "    if relevant_count < len(documents) / 2:\n",
    "        next_steps = \"Try rephrasing your question, or adding more documents related to the question.\"\n",
    "    else:\n",
    "        next_steps = \"\"\n",
    "    \n",
    "    relevance_report_msg = f\"\"\"\n",
    "        <p style=\"font-size: large;\">\n",
    "            Found <span style=\"color: black;\">{relevant_count}</span> out of <span style=\"color: black;\">{len(documents)}</span> documents to be most relevant to the question.\n",
    "        </p>\n",
    "        <p style=\"font-size: large\">\n",
    "            {next_steps}\n",
    "        </p>\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search, \"relevant_count\": relevant_count}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e9ad6-3ca4-4837-b3b4-4996265316e3",
   "metadata": {},
   "source": [
    "### Answer Reliability meter function setup\n",
    "The logic here is that more graded relevant document snippets will typically lead to a more reliable answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3be2775-df88-44b8-b9c4-ecd3eb6d9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_reliability_meter(relevant_count):\n",
    "    global k_nearest\n",
    "\n",
    "    # Increase the number of blocks by a factor of 4 to size the meter and make it look more visible\n",
    "    relevant_count *= 4\n",
    "    # relevant_count = min(relevant_count * 4, 20)  # Limit to 25 blocks max\n",
    "    local_k_nearest = k_nearest * 3\n",
    "\n",
    "    # Calculate relevance percentage based on the scaled values\n",
    "    relevance_percentage = min((relevant_count / local_k_nearest) * 100, 100)\n",
    "    \n",
    "    if relevant_count > local_k_nearest / 2:\n",
    "        # High relevance: more than half of local_k_nearest are relevant\n",
    "        filled_square = \"<span style='color: green;'>&#9632;</span>\" * relevant_count  # Filled square: ■\n",
    "        empty_square = \"<span style='color: lightgray;'>&#9633;</span>\" * (local_k_nearest - relevant_count)  # Empty square: □\n",
    "    else:\n",
    "        # Low relevance: less than or equal to half of local_k_nearest are relevant\n",
    "        filled_square = \"<span style='color: yellow;'>&#9632;</span>\" * relevant_count  # Yellow square: ■\n",
    "        empty_square = \"<span style='color: lightgray;'>&#9633;</span>\" * (local_k_nearest - relevant_count)  # Empty square: □\n",
    "\n",
    "    # return f\"\"\"\n",
    "    # <div style='font-size: 24px; display: flex; align-items: center; justify-content: space-between; \n",
    "    #             width: 100%; max-width: 500px; white-space: nowrap; overflow: hidden;'>\n",
    "    #     <div style='display: flex; gap: 2px; max-width: 420px; overflow: hidden; flex-shrink: 0;'>{filled_square}{empty_square}</div>\n",
    "    #     <span style='font-size: 18px; margin-left: 8px;'>{relevance_percentage:.0f}%</span>\n",
    "    # </div>\"\"\"\n",
    "\n",
    "\n",
    "    # # Return the meter with a percentage display without fixing the width\n",
    "    # return f\"\"\"\n",
    "    # <div style='font-size: 24px; display: flex; flex-wrap: wrap; align-items: center; width: 100%; max-width: 400px;'>\n",
    "    #     {filled_square + empty_square}\n",
    "    #     <span style='font-size: 18px; margin-left: 10px;'>{relevance_percentage:.0f}%</span>\n",
    "    # </div>\"\"\"\n",
    "\n",
    "    return f\"<div style='font-size: 24px; display: flex; align-items: center; justify-content: space-between; width: 100%;'>\" \\\n",
    "           f\"{filled_square + empty_square}\" \\\n",
    "           f\"<span style='font-size: 18px;'>{relevance_percentage:.0f}%</span>\" \\\n",
    "           f\"</div>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009dde9-de76-477b-a1ec-83d939aaa29f",
   "metadata": {},
   "source": [
    "### Decide to Generate or web fallback function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb07cd5b-9276-43e2-925c-b28e2263b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    global web_fallback_status\n",
    "    \n",
    "    if web_search == \"Yes\":\n",
    "        # No relevant documents were found, so fall back to web search\n",
    "        print(\n",
    "            \"---DECISION: RAG DOCS DO NOT CONTAIN RELEVANT CONTENT, FALLING BACK TO WEBSEARCH---\"\n",
    "        )\n",
    "        web_fallback_status = \"success\"\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate the answer\n",
    "        print(\"---DECISION: GENERATE ANSWER---\")\n",
    "        return \"generate\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd3dd6ba-1f10-43be-b34c-19616157d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Check if 'documents' key exists in state, if not, initialize it\n",
    "    if \"documents\" not in state:\n",
    "        state[\"documents\"] = []\n",
    "\n",
    "    ## passes existing documents list to function, there may or may not be doc in the array\n",
    "    \n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "    # Transform the keys from 'url' to 'source' and 'content' to 'page_content'\n",
    "    transformed_docs = [{\"source\": d[\"url\"], \"page_content\": d[\"content\"]} for d in docs]\n",
    "\n",
    "    # Create Document objects with the transformed results\n",
    "    for doc in transformed_docs:\n",
    "        document = Document(metadata={'source': doc['source']}, page_content=doc['page_content'])\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    # # Join the transformed documents into a single string\n",
    "    # web_results = \"\\n\".join([f\"source: {d['source']}\\npage_content: {d['page_content']}\" for d in transformed_docs])\n",
    "\n",
    "    # web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    # web_results = Document(page_content=web_results)\n",
    "\n",
    "    ## adds the web results to the existing documents list\n",
    "    \n",
    "    # documents.append(web_results)\n",
    "\n",
    "    # Status message\n",
    "    global websearch_status\n",
    "    websearch_status = \"success\"\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ee3fb-0448-4710-8d51-eee35b716da7",
   "metadata": {},
   "source": [
    "### Web search function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6988645c-4858-4a65-8d61-1322789bc898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Check if 'documents' key exists in state, if not, initialize it\n",
    "    if \"documents\" not in state:\n",
    "        state[\"documents\"] = []\n",
    "\n",
    "    ## passes existing documents list to function, there may or may not be doc in the array\n",
    "    \n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "    \n",
    "    # Transform the keys from 'url' to 'source' and 'content' to 'page_content'\n",
    "    transformed_docs = [{\"source\": d[\"url\"], \"page_content\": d[\"content\"]} for d in docs]\n",
    "\n",
    "    # Create Document objects with the transformed results\n",
    "    for doc in transformed_docs:\n",
    "        document = Document(page_content=doc['page_content'], metadata={'source': doc['source']})\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    \n",
    "    global relevance_report_msg\n",
    "    \n",
    "    relevance_report_msg = f\"\"\"\n",
    "            <p style=\"font-size: large;\">\n",
    "                RAG database lacked relevant documents, Agent has diverted question to Web Search.\n",
    "            </p>\n",
    "        \"\"\"\n",
    "    \n",
    "    global relevant_count, k_nearest, websearch_status\n",
    "    \n",
    "    relevant_count = len(documents)\n",
    "    k_nearest = len(documents)\n",
    "    websearch_status = \"success\"\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572960c4-8945-4fac-9f7e-d61e9432cac0",
   "metadata": {},
   "source": [
    "### Generate answer function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b098a25b-04fc-431d-a2e9-96a8b2d78789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE AN ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03f3c0-6edf-4550-916f-8e33bfa75ccf",
   "metadata": {},
   "source": [
    "### Calculate NVIDIA green rank sources function setup\n",
    "this is for the GUI, the rerank source docs list have a descending shade of green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8095d6d-5099-4bb4-912c-bfd12d3e152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nvidia_green_intensity(rank, max_rank=1):\n",
    "    \"\"\"\n",
    "    Adjust the green intensity based on rank.\n",
    "    - If there's only 1 document, default to medium green.\n",
    "    - If multiple documents exist, Rank 1 is lightest, and higher ranks get darker.\n",
    "    \"\"\"\n",
    "    nvidia_green_base = (118, 185, 0)  # Base NVIDIA green (#76B900)\n",
    "\n",
    "    # Handle the case where there's only one document (avoid division by zero)\n",
    "    if max_rank == 1:\n",
    "        green_intensity_factor = 0.75  # Default to medium green if only 1 document\n",
    "    else:\n",
    "        normalized_rank = (rank - 1) / (max_rank - 1)  # Rank 1 -> 0 (lightest), max_rank -> 1 (darkest)\n",
    "        green_intensity_factor = 0.5 + (normalized_rank * 0.5)  # Gradual shift to darker green\n",
    "\n",
    "    # Apply the adjusted green intensity\n",
    "    green_intensity = tuple(int(value * green_intensity_factor) for value in nvidia_green_base)\n",
    "\n",
    "    return f'rgb({green_intensity[0]}, {green_intensity[1]}, {green_intensity[2]})'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919abb2-2c7c-4372-ad7c-b67ee8d75c13",
   "metadata": {},
   "source": [
    "### Usefulness Grade answer vs documents vs question function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07fa3d08-6a86-4705-a28b-e2721070bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Conditional Edge ##############################\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---HALLUCINATION CHECKER---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    ## SOURCE DOCUMENTS HANDLER ##\n",
    "    ## create a new array with source file and page content snippet for use in GUI output\n",
    "    \n",
    "    global filtered_docs_formatted\n",
    "    filtered_docs_formatted = []\n",
    "\n",
    "    for rank, doc in enumerate(documents, start=1):  # Ensure documents is defined\n",
    "        source = doc.metadata['source']\n",
    "        page_content_snippet = doc.page_content[:200]  # Get the first 200 characters of the snippet\n",
    "        color = calculate_nvidia_green_intensity(rank, max_rank=len(documents))  # Dynamic rank-based green\n",
    "\n",
    "        # Append the formatted HTML to the list\n",
    "        filtered_docs_formatted.append(f'''\n",
    "        <table style=\"width: 100%; border-collapse: collapse; background-color: #fff; margin-bottom: 5px;\">\n",
    "            <tr>\n",
    "                <td style=\"width: 5%; background-color: {color}; text-align: center; font-weight: bold; color: white; padding: 5px;\">\n",
    "                    {rank}\n",
    "                </td>\n",
    "                <td style=\"padding: 5px;\">\n",
    "                    <div style=\"font-weight: bold;\">Source:</div>\n",
    "                    <a href=\"{source}\" target=\"_blank\" class=\"custom-link\">{source}</a>\n",
    "                    <br>\n",
    "                    <div style=\"margin-top: 5px;\"><b>Snippet</b>: {page_content_snippet}</div>\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        ''')\n",
    "\n",
    "\n",
    "    \n",
    "    ### HALLUCINATION CHECK\n",
    "    ### first starts with hallucination grader which compares the answer to the documents\n",
    "    ### a grade of YES means grounded in documents.  \n",
    "    ### a grade of NO would indicate not grounded in docs and would qualify as an hallucination.\n",
    "\n",
    "    ### USEFULNESS CHECK\n",
    "    ### then a usefulness check that compares the answer to the question to ensure it actually answers the question.\n",
    "    ### a grade of YES means the answer addresses the question \n",
    "    ### a grade of NO would indicate the answer fails to address the question.\n",
    "\n",
    "\n",
    "    # Status message\n",
    "    global hallucination_status, usefulness_status, formatted_usefulness_table\n",
    "    \n",
    "    # HTML table template\n",
    "    usefulness_table_template = \"\"\"\n",
    "    <table border=\"1\">\n",
    "      <tr>\n",
    "        <th>Status</th>\n",
    "        <th>Task</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"color: {color1};\">{status1}</td>\n",
    "        <td>Answer is grounded in relevant documents</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"color: {color2};\">{status2}</td>\n",
    "        <td>Answer effectively addresses the question</td>\n",
    "      </tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "    \n",
    "    # Check whether or not answer is grounded in documents and no hallucinations, yes is pass, no is fail.\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: ANSWER IS GROUNDED IN DOCUMENTS - NO HALLUCINATIONS---\")\n",
    "    \n",
    "        hallucination_status = \"success\"\n",
    "    \n",
    "        # After hallucination check has passed, now check whether the answer addresses the question\n",
    "        print(\"---GRADE ANSWER vs THE QUESTION---\")\n",
    "        \n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        \n",
    "        grade = score[\"score\"]\n",
    "    \n",
    "        # Evaluate the question-answering score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: ANSWER ADDRESSES QUESTION AND IS USEFUL---\")\n",
    "            usefulness_status = \"success\"\n",
    "            \n",
    "            # Update the HTML table\n",
    "            formatted_usefulness_table = usefulness_table_template.format(color1=\"green\", status1=\"&#10004;\", color2=\"green\", status2=\"&#10004;\")\n",
    "            \n",
    "            return \"useful\"\n",
    "            \n",
    "        else:\n",
    "            print(\"---DECISION: ANSWER DOES NOT ADDRESS QUESTION---\")\n",
    "            \n",
    "            # Update the HTML table\n",
    "            formatted_usefulness_table = usefulness_table_template.format(color1=\"green\", status1=\"&#10004;\", color2=\"grey\", status2=\"&#10008;\")\n",
    "            \n",
    "            return \"not useful\"\n",
    "    \n",
    "    # If it's hallucinating, and answer is not related to documents, retry\n",
    "    else:\n",
    "        print(\"---DECISION: POSSIBLE HALLUCINATIONS - ANSWER IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        \n",
    "        # Update the HTML table\n",
    "        formatted_usefulness_table = usefulness_table_template.format(color1=\"grey\", status1=\"&#10008;\", color2=\"grey\", status2=\"&#10008;\")\n",
    "        \n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8cd2a5-8d52-473d-92bb-8c3aa6e0218d",
   "metadata": {},
   "source": [
    "### Langgraph node definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b1f20fc-12b2-4bfd-b551-779689e382c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"rerank\", rerank)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f21594-00d4-48a8-ae2e-4e55a010b540",
   "metadata": {},
   "source": [
    "### Langgraph build node relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9a4b9e4-3ba8-47d6-958c-e5a7112ac6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"rerank\")\n",
    "workflow.add_edge(\"rerank\", \"grade_documents\")\n",
    "\n",
    "# workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709e8cc-2df2-431f-a361-2f38cd40db21",
   "metadata": {},
   "source": [
    "# Agentic Response Function for GUI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a059f26-b861-402a-bcc1-d14bde4ce551",
   "metadata": {},
   "source": [
    "### Status panel formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "177e6913-c682-4fb5-ae64-30e89367b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import time\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "### used to format and color the status alert\n",
    "\n",
    "def status_update(status):\n",
    "    if status == \"success\":\n",
    "        return '<div style=\"background-color: green; color: white; padding: 5px; border-radius: 5px;\">Completed successfully</div>'\n",
    "    elif status in [\"websearch\", \"vectorstore\"]:\n",
    "        return f'<div style=\"background-color: blue; color: white; padding: 5px; border-radius: 5px;\">{status}</div>'\n",
    "    else:\n",
    "        return '<div style=\"background-color: grey; color: white; padding: 5px; border-radius: 5px;\">Not used</div>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62adf4d-11da-4926-9f82-238e1c78982b",
   "metadata": {},
   "source": [
    "### MAIN RESPONSE FUNCTION\n",
    "This takes the inputs from the GUI and triggers all the other external functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "863fbb02-fd75-40aa-8560-88e8e78adff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agentic_response(question, mode_toggle):\n",
    "\n",
    "   \n",
    "    ### Create and set status globals\n",
    "    ### reset value of previous variable values upon new execution of main function\n",
    "    \n",
    "    global router_status, router_choice, retrieve_status, rerank_status, relevance_status, web_fallback_status, websearch_status, hallucination_status, usefulness_status, filtered_docs_formatted, answer_reliability_content, routing_agent_panel_content, relevance_report_msg, formatted_usefulness_table\n",
    "    \n",
    "    # router_status = None\n",
    "    # router_choice = None\n",
    "    # retrieve_status = None\n",
    "    # rerank_status = None\n",
    "    # relevance_status = None\n",
    "    # web_fallback_status = None\n",
    "    # websearch_status = None\n",
    "    \n",
    "    hallucination_status = None\n",
    "    usefulness_status = None\n",
    "    routing_agent_panel_content = None\n",
    "    relevance_report_msg = None\n",
    "    answer_reliability_content = None\n",
    "    \n",
    "\n",
    "    # print(routing_agent_panel_content)\n",
    "\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile()\n",
    "\n",
    "    # Prepare the input\n",
    "    inputs = {\"question\": question}\n",
    "\n",
    "    # Initialize the response variable\n",
    "    response = None\n",
    "\n",
    "\n",
    "    # Stream the output from the app\n",
    "\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            # Check if 'generation' key is in the value\n",
    "            if 'generation' in value:\n",
    "                response = value['generation']\n",
    "\n",
    "    \n",
    "    graded_response = f\"Response:\\n{response}\"\n",
    "\n",
    "    ### bring in filtered docs formatted from outside function, join the contents to make it look better in textbox in GUI\n",
    "    \n",
    "    filtered_docs_content = \"\\n\\n\".join(filtered_docs_formatted)\n",
    "\n",
    "    \n",
    "    \n",
    "    # bring in Status messages for indicator panel\n",
    "\n",
    "    hallucination_status_result = status_update(hallucination_status)\n",
    "    usefulness_status_result = status_update(usefulness_status)\n",
    "\n",
    "\n",
    "    # if using LLM-toggle turn off the agent status messages since they aren't used\n",
    "    if mode_toggle == \"LLM only mode\":\n",
    "        llm_response = get_llm_response(question)\n",
    "        # router_status_result = \"Not used\"\n",
    "        # router_choice_result = \"Not used\"\n",
    "        # retrieve_status_result = \"Not used\"\n",
    "        # rerank_status_result = \"Not used\"\n",
    "        # relevance_status = \"Not used\"\n",
    "        # web_fallback_status = \"Not used\"\n",
    "        # websearch_status = \"Not used\"\n",
    "        # hallucination_status = \"Not used\"\n",
    "        # usefulness_status = \"Not used\"\n",
    "        # relevance_status_result = \"Not used\"\n",
    "        # web_fallback_status_result = \"Not used\"\n",
    "        # websearch_status_result = \"Not used\"\n",
    "        \n",
    "        hallucination_status_result = \"Not used\"\n",
    "        usefulness_status_result = \"Not used\"\n",
    "        relevance_report_msg = \"LLM only, no relevance check\"\n",
    "        llm_only_source_result = \"LLM only, no source docs\"\n",
    "        routing_agent_panel_content = \"LLM only, no routing\"\n",
    "        answer_reliability_content = \"LLM only, cannot verify answer\"\n",
    "        formatted_usefulness_table = \"LLM only, cannot verify answer\"\n",
    "        \n",
    "        return ( \n",
    "            llm_response, \n",
    "            routing_agent_panel_content, \n",
    "            answer_reliability_content, \n",
    "            relevance_report_msg, \n",
    "            hallucination_status_result, \n",
    "            usefulness_status_result, \n",
    "            formatted_usefulness_table, \n",
    "            llm_only_source_result\n",
    "        )\n",
    "\n",
    "                     \n",
    "\n",
    "    \n",
    "    answer_reliability_content = answer_reliability_meter(relevant_count)\n",
    "        \n",
    "###  the return statement of the function needs to return in the order that matches the outputs in gradio]\n",
    "### in this case we are going from top left to left bottom.  To top right, to right bottom.\n",
    "\n",
    "    return (\n",
    "        response, \n",
    "        routing_agent_panel_content, \n",
    "        answer_reliability_content, \n",
    "        relevance_report_msg, \n",
    "        hallucination_status_result, \n",
    "        usefulness_status_result, \n",
    "        formatted_usefulness_table, \n",
    "        filtered_docs_content\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42229271-2511-4d2a-8b4d-92d941977f74",
   "metadata": {},
   "source": [
    "### Example question array for GUI Example questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83f267b1-aa65-4d4d-a964-8cd8343199db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit data below for specific demos ----\n",
    "EXAMPLE_TITLES = [\n",
    "                     \"### Vector Search\",\n",
    "                     \"### Web Fallback\",\n",
    "                     \"### Web Search\",\n",
    "                 ]\n",
    "EXAMPLES = [\n",
    "\n",
    "###Vector Search\n",
    "\n",
    "               [\n",
    "                   \n",
    "        \"Create an email to the head nurse that summarizes the patients admitted for heart related issues.\",\n",
    "        \"What can you say about sunscreen effectiveness in preventing melanoma?\",\n",
    "        \"Please summarize the clinical trial info we have on our drug ipilimumab for melanoma.\",\n",
    "        \"What are the key domains of population-based approaches to mental health?\",\n",
    "\n",
    "               ],\n",
    "\n",
    "### Web Fallback\n",
    "\n",
    "               [\n",
    "        \"What are the FDA-approved treatments for skin cancer in 2024?\",\n",
    "\n",
    "               ],\n",
    "\n",
    "### Web Search\n",
    "\n",
    "\t\t       [ \n",
    "        \"What year did the Bears football team win the super bowl?\",\n",
    "        \"What is the chemical makeup of water?\"\n",
    "               ],\n",
    "\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728eeb9-9312-488f-a6e3-f68e517ad2af",
   "metadata": {},
   "source": [
    "# GUI setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d7794-c52c-4b56-a66e-f9febf993f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import base64\n",
    "\n",
    "# Placeholder URLs for the logos\n",
    "DELL_LOGO_URL = \"https://upload.wikimedia.org/wikipedia/commons/4/48/Dell_Logo.svg\"\n",
    "NVIDIA_LOGO_URL = \"https://upload.wikimedia.org/wikipedia/commons/a/a4/NVIDIA_logo.svg\"\n",
    "\n",
    "# Load the NVIDIA logo using base64 encoding\n",
    "def get_image_base64(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Path to your local NVIDIA logo\n",
    "nvidia_logo_path = os.path.join(\"images\", \"nvidia-logo.png\")\n",
    "nvidia_base64 = get_image_base64(nvidia_logo_path)\n",
    "dell_logo_path = os.path.join(\"images\", \"dell-logo.png\")\n",
    "dell_base64 = get_image_base64(dell_logo_path)\n",
    "\n",
    "def clear_fields():\n",
    "    return \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "def check_question(question):\n",
    "    if not question.strip():\n",
    "        gr.Warning(\"No question entered, please input a question.\")\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(), title=\"Health Clinic Assistant\") as demo:\n",
    "\n",
    "    # Custom CSS for styling\n",
    "    style = '''\n",
    "    <style>\n",
    "        /* Common styles for both modes */\n",
    "        .custom-html {\n",
    "            border-radius: 5px;\n",
    "            padding: 8px;\n",
    "            height: 180px;\n",
    "            overflow-y: auto;\n",
    "        }\n",
    "        .logo-container {\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "        }\n",
    "        .logo-container img {\n",
    "            height: 40px;\n",
    "            width: auto;\n",
    "            margin-right: 15px;\n",
    "        }\n",
    "        .custom-link {\n",
    "            text-decoration: none;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        .spaced-column {\n",
    "            padding-right: 10px; /* Adds space between columns */\n",
    "        }\n",
    "        .status-panel {\n",
    "            border-radius: 5px;\n",
    "            padding: 8px;\n",
    "            height: 50px;\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            justify-content: center;\n",
    "            width: 100%;\n",
    "            text-align: center;\n",
    "        }\n",
    "        \n",
    "        /* Light mode - using Gradio's light theme class */\n",
    "        .light-theme .custom-html {\n",
    "            border: 1px solid #ccc;\n",
    "            background: #fff;\n",
    "        }\n",
    "        .light-theme body, .light-theme .gradio-container {\n",
    "            background-color: #f4f4f4;\n",
    "            color: #333;\n",
    "            font-family: Arial, sans-serif;\n",
    "        }\n",
    "        .light-theme .custom-link {\n",
    "            color: #76B900;\n",
    "        }\n",
    "        .light-theme .status-panel {\n",
    "            border: 2px solid #ccc;\n",
    "            background-color: white;\n",
    "            color: #333;\n",
    "        }\n",
    "        \n",
    "        /* Dark mode - using Gradio's dark theme class */\n",
    "        .dark-theme .custom-html {\n",
    "            border: 1px solid #444;\n",
    "            background: #2d2d2d;\n",
    "        }\n",
    "        .dark-theme body, .dark-theme .gradio-container {\n",
    "            background-color: #1a1a1a;\n",
    "            color: white;\n",
    "            font-family: Arial, sans-serif;\n",
    "        }\n",
    "        .dark-theme .custom-link {\n",
    "            color: #9eff00;\n",
    "        }\n",
    "        .dark-theme .status-panel {\n",
    "            border: 2px solid #444;\n",
    "            background-color: #2d2d2d;\n",
    "            color: white;\n",
    "        }\n",
    "        /* These important flags ensure text inputs are readable in dark mode */\n",
    "        .dark-theme input, \n",
    "        .dark-theme textarea, \n",
    "        .dark-theme select, \n",
    "        .dark-theme .gradio-textbox textarea, \n",
    "        .dark-theme .gradio-textbox input {\n",
    "            color: white !important;\n",
    "            background-color: #2d2d2d !important;\n",
    "        }\n",
    "        /* Make sure buttons have proper contrast */\n",
    "        .dark-theme button, \n",
    "        .dark-theme .gradio-button {\n",
    "            background-color: #2d2d2d !important;\n",
    "            color: white !important;\n",
    "            border-color: #444 !important;\n",
    "        }\n",
    "    </style>\n",
    "    '''\n",
    "    gr.HTML(style)\n",
    "\n",
    "    # TITLE and LOGOS\n",
    "    with gr.Row():\n",
    "        gr.HTML(f\"\"\"\n",
    "        <div class=\"logo-container\">\n",
    "            <img src=\"data:image/png;base64,{dell_base64}\" alt=\"Dell Logo\">            \n",
    "            <img src=\"data:image/png;base64,{nvidia_base64}\" alt=\"NVIDIA Logo\">\n",
    "        </div>\n",
    "        <h2>Gen AI Health Assistant - Dell Technologies & NVIDIA</h2>\n",
    "        <p>Dataset contains journals on COVID, Skin Cancer and Mental Health as well as simulated clinic patient data</p>\n",
    "        \"\"\")\n",
    "    \n",
    "    # MAIN ROW AFTER TITLE\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "\n",
    "            # QUESTION DROPDOWN\n",
    "            question_dropdown = gr.Dropdown(choices=[question for section in EXAMPLES for question in section],\n",
    "                                            label=\"Select a Question\", \n",
    "                                            interactive=True)\n",
    "\n",
    "    \n",
    "            # MODE\n",
    "            mode_toggle = gr.Dropdown(choices=[\"Agentic RAG mode\", \"LLM only mode\"], \n",
    "                                      label=\"Select Mode\", value=\"Agentic RAG mode\", \n",
    "                                      interactive=True)\n",
    "            \n",
    "            # QUESTION\n",
    "            question = gr.Textbox(label=\"Prompt\", placeholder=\"Enter your question here...\", lines=2, max_lines=2)\n",
    "\n",
    "\n",
    "            # Auto-populate selected question into the textbox\n",
    "            question_dropdown.change(\n",
    "                fn=lambda q: q,\n",
    "                inputs=[question_dropdown],\n",
    "                outputs=[question]\n",
    "            )\n",
    "\n",
    "            # BUTTONS\n",
    "            with gr.Row():  \n",
    "                submit_button = gr.Button(\"Submit\")\n",
    "                clear_button = gr.Button(\"Clear\")\n",
    "                stop_btn = gr.Button(\"Stop Process\")\n",
    "\n",
    "            # RESPONSE\n",
    "            response = gr.Textbox(label=\"Response\", lines=16, max_lines=16)\n",
    "\n",
    "        ################### RIGHT COLUMN\n",
    "        with gr.Column(scale=1):  \n",
    "\n",
    "            # ROW FOR STATUS PANELS (Reliability Meter, Hallucination, Usefulness) - FIXED\n",
    "            with gr.Row(equal_height=True):\n",
    "                with gr.Column(scale=4, min_width=270, elem_classes=\"spaced-column\"):\n",
    "                    gr.Markdown(\"#### Answer Reliability Meter\")\n",
    "                    answer_reliability_content = gr.HTML(\"<div class='status-panel'>Reliability Content</div>\")\n",
    "                \n",
    "                with gr.Column(scale=2, min_width=100, elem_classes=\"spaced-column\"):\n",
    "                    gr.Markdown(\"#### Hallucination Check\")\n",
    "                    hallucination_status_result = gr.HTML(\"<div class='status-panel'>Hallucination Check Result</div>\")\n",
    "                \n",
    "                with gr.Column(scale=2, min_width=100):\n",
    "                    gr.Markdown(\"#### Usefulness Check\")\n",
    "                    usefulness_status_result = gr.HTML(\"<div class='status-panel'>Usefulness Check Result</div>\")\n",
    "\n",
    "\n",
    "            gr.Markdown(\"<b>Routing Agent Panel</b>\")\n",
    "            with gr.Accordion(\"See Details\", open=False):  \n",
    "                routing_agent_panel_content = gr.HTML(\"<div>Routing Agent Content</div>\")\n",
    "                relevance_report_msg = gr.HTML(\"<div>Relevance Report Content</div>\")\n",
    "\n",
    "            gr.Markdown(\"<b>Rerank Agent Sources and Scores</b>\")\n",
    "            with gr.Accordion(\"See Details\", open=False):  \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        filtered_docs_content = gr.HTML(\"<div>Rerank Agent Sources and Scores Content</div>\")\n",
    "\n",
    "            gr.Markdown(\"#### Usefulness Table\")\n",
    "            formatted_usefulness_table = gr.HTML(\"<div>Usefulness Table Content</div>\")\n",
    "\n",
    "    gr.Markdown(\"<hr>\")\n",
    "    gr.Markdown(\"<hr>\")\n",
    "    \n",
    "    warning_popup = gr.HTML(\"<div style='color: red;'>Please input question</div>\", visible=False)\n",
    "\n",
    "    start_event = submit_button.click(\n",
    "        get_agentic_response, \n",
    "        inputs=[question, mode_toggle], \n",
    "        outputs=[response,\n",
    "                 routing_agent_panel_content,\n",
    "                 answer_reliability_content,\n",
    "                 relevance_report_msg,\n",
    "                 hallucination_status_result, \n",
    "                 usefulness_status_result,\n",
    "                 formatted_usefulness_table,\n",
    "                 filtered_docs_content,\n",
    "                ]\n",
    "    )\n",
    "    \n",
    "    submit_button.click(\n",
    "        check_question, \n",
    "        inputs=[question], \n",
    "        outputs=[warning_popup]\n",
    "    )\n",
    "    \n",
    "    clear_button.click(\n",
    "        clear_fields, \n",
    "        inputs=[], \n",
    "        outputs=[response,\n",
    "                 routing_agent_panel_content,\n",
    "                 answer_reliability_content,\n",
    "                 relevance_report_msg,\n",
    "                 hallucination_status_result, \n",
    "                 usefulness_status_result,\n",
    "                 formatted_usefulness_table,\n",
    "                 filtered_docs_content,\n",
    "                 warning_popup\n",
    "                ]\n",
    "    )    \n",
    "    \n",
    "    stop_btn.click(\n",
    "        fn=None, \n",
    "        inputs=None, \n",
    "        outputs=None, \n",
    "        cancels=[start_event]\n",
    "    )\n",
    "\n",
    "demo.queue(max_size=25)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "images_path = os.path.join(current_dir, \"images\")\n",
    "\n",
    "demo.launch(share=False, debug=True, server_name=\"0.0.0.0\", server_port=7869, allowed_paths=[images_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61410cc6-ab5d-4ecf-8931-c8d2e49857fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
